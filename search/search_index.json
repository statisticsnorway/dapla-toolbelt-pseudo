{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pseudonymization extensions for Dapla Toolbelt","text":"<p>Pseudonymize, repseudonymize and depseudonymize data on Dapla.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Dapla Toolbelt</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install dapla-toolbelt-pseudo via pip from PyPI:</p> <pre><code>python -m pip install dapla-toolbelt-pseudo\n</code></pre>"},{"location":"#basic-usage","title":"Basic usage","text":""},{"location":"#pseudonymize","title":"Pseudonymize","text":"<pre><code>from dapla_pseudo import PseudoData\nimport pandas as pd\n\nfile_path=\"data/personer.json\"\n\ndf = pd.read_json(file_path) # Create DataFrame from file\n\n# Example: Single field default encryption (DAEAD)\nresult_df = (\n    PseudoData.from_pandas(df)                     # Specify what dataframe to use\n    .on_field(\"fornavn\")                           # Select the field to pseudonymize\n    .pseudonymize()                                # Apply pseudonymization to the selected field\n    .to_polars()                                   # Get the result as a polars dataframe\n)\n\n# Example: Multiple fields default encryption (DAEAD)\nresult_df = (\n    PseudoData.from_pandas(df)                     # Specify what dataframe to use\n    .on_fields(\"fornavn\", \"etternavn\")             # Select multiple fields to pseudonymize\n    .pseudonymize()                                # Apply pseudonymization to the selected fields\n    .to_polars()                                   # Get the result as a polars dataframe\n)\n\n# Example: Single field sid mapping and pseudonymization (FPE)\nresult_df = (\n    PseudoData.from_pandas(df)                     # Specify what dataframe to use\n    .on_field(\"fnr\")                               # Select the field to pseudonymize\n    .map_to_stable_id()                            # Map the selected field to stable id\n    .pseudonymize()                                # Apply pseudonymization to the selected fields\n    .to_polars()                                   # Get the result as a polars dataframe\n)\n</code></pre> <p>The default encryption algorithm is DAEAD (Deterministic Authenticated Encryption with Associated Data). However, if the field is a valid Norwegian personal identification number (fnr, dnr), the recommended way to pseudonymize is to use the function <code>map_to_stable_id()</code> to convert the identification number to a stable ID (SID) prior to pseudonymization. In that case, the pseudonymization algorithm is FPE (Format Preserving Encryption).</p>"},{"location":"#validate-sid-mapping","title":"Validate SID mapping","text":"<pre><code>from dapla_pseudo import Validator\nimport pandas as pd\n\nfile_path=\"data/personer.json\"\n\ndf = pd.read_json(file_path)\n\nresult = (\n    Validator.from_pandas(df)                   # Specify what dataframe to use\n    .on_field(\"fnr\")                            # Select the field to validate\n    .validate_map_to_stable_id()                # Validate that all the field values can be mapped to a SID\n)\n# The resulting dataframe contains the field values that didn't have a corresponding SID\nresult.to_pandas()\n</code></pre> <p>A <code>sid_snapshot_date</code> can also be specified to validate that the field values can be mapped to a SID at a specific date:</p> <pre><code>from dapla_pseudo import Validator\nfrom dapla_pseudo.utils import convert_to_date\nimport pandas as pd\n\nfile_path=\"data/personer.json\"\n\ndf = pd.read_json(file_path)\n\nresult = (\n    Validator.from_pandas(df)\n    .on_field(\"fnr\")\n    .validate_map_to_stable_id(\n        sid_snapshot_date=convert_to_date(\"2023-08-29\")\n    )\n)\n# Show metadata about the validation (e.g. which version of the SID catalog was used)\nresult.metadata\n# Show the field values that didn't have a corresponding SID\nresult.to_pandas()\n</code></pre>"},{"location":"#advanced-usage","title":"Advanced usage","text":""},{"location":"#pseudonymize_1","title":"Pseudonymize","text":""},{"location":"#read-from-file-systems","title":"Read from file systems","text":"<pre><code>from dapla_pseudo import PseudoData\nfrom dapla import AuthClient\n\n\nfile_path=\"data/personer.json\"\n\noptions = {\n    # Specify data types of columns in the dataset\n    \"dtype\" : { \"fnr\": \"string\",\"fornavn\": \"string\",\"etternavn\": \"string\",\"kjonn\": \"category\",\"fodselsdato\": \"string\"}\n}\n\n# Example: Read dataframe from file\nresult_df = (\n    PseudoData.from_file(file_path, **options)     # Read the DataFrame from file\n    .on_fields(\"fornavn\", \"etternavn\")             # Select multiple fields to pseudonymize\n    .pseudonymize()                                # Apply pseudonymization to the selected fields\n    .to_polars()                                   # Get the result as a polars dataframe\n)\n\n# Example: Read dataframe from GCS bucket\noptions = {\n    # Specify data types of columns in the dataset\n    \"dtype\" : { \"fnr\": \"string\",\"fornavn\": \"string\",\"etternavn\": \"string\",\"kjonn\": \"category\",\"fodselsdato\": \"string\"},\n    # Specify storage options for Google Cloud Storage (GCS)\n    \"storage_options\" : {\"token\": AuthClient.fetch_google_credentials()}\n}\n\ngcs_file_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/pseudo-examples/andeby_personer.csv\"\n\nresult_df = (\n    PseudoData.from_file(gcs_file_path, **options) # Read DataFrame from GCS\n    .on_fields(\"fornavn\", \"etternavn\")             # Select multiple fields to pseudonymize\n    .pseudonymize()                                # Apply pseudonymization to the selected fields\n    .to_polars()                                   # Get the result as a polars dataframe\n)\n</code></pre>"},{"location":"#pseudoyminize-using-a-custom-keyset","title":"Pseudoyminize using a custom keyset","text":"<pre><code>from dapla_pseudo import pseudonymize\n\n# Pseudonymize fields in a local file using the default key:\npseudonymize(file_path=\"./data/personer.json\", fields=[\"fnr\", \"fornavn\"])\n\n# Pseudonymize fields in a local file, explicitly denoting the key to use:\npseudonymize(file_path=\"./data/personer.json\", fields=[\"fnr\", \"fornavn\"], key=\"ssb-common-key-1\")\n\n# Pseudonymize a local file using a custom key:\nimport json\ncustom_keyset = json.dumps({\n    \"encryptedKeyset\": \"CiQAp91NBhLdknX3j9jF6vwhdyURaqcT9/M/iczV7fLn...8XYFKwxiwMtCzDT6QGzCCCM=\",\n    \"keysetInfo\": {\n        \"primaryKeyId\": 1234567890,\n        \"keyInfo\": [\n            {\n                \"typeUrl\": \"type.googleapis.com/google.crypto.tink.AesSivKey\",\n                \"status\": \"ENABLED\",\n                \"keyId\": 1234567890,\n                \"outputPrefixType\": \"TINK\",\n            }\n        ],\n    },\n    \"kekUri\": \"gcp-kms://projects/some-project-id/locations/europe-north1/keyRings/some-keyring/cryptoKeys/some-kek-1\",\n})\npseudonymize(file_path=\"./data/personer.json\", fields=[\"fnr\", \"fornavn\"], key=custom_keyset)\n\n# Operate on data in a streaming manner:\nimport shutil\nwith pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer_deid.json\", 'wb') as f:\n        res.raw.decode_content = True\n        shutil.copyfileobj(res.raw, f)\n\n# Map certain fields to stabil ID\npseudonymize(file_path=\"./data/personer.json\", fields=[\"fornavn\"], sid_fields=[\"fnr\"])\n</code></pre>"},{"location":"#repseudonymize","title":"Repseudonymize","text":"<pre><code>from dapla_pseudo import repseudonymize\n\n# Repseudonymize fields in a local file, denoting source and target keys to use:\nrepseudonymize(file_path=\"./data/personer_deid.json\", fields=[\"fnr\", \"fornavn\"], source_key=\"ssb-common-key-1\", target_key=\"ssb-common-key-2\")\n</code></pre>"},{"location":"#depseudonymize","title":"Depseudonymize","text":"<pre><code>from dapla_pseudo import depseudonymize\n\n# Depseudonymize fields in a local file using the default key:\ndepseudonymize(file_path=\"./data/personer_deid.json\", fields=[\"fnr\", \"fornavn\"])\n\n# Depseudonymize fields in a local file, explicitly denoting the key to use:\ndepseudonymize(file_path=\"./data/personer_deid.json\", fields=[\"fnr\", \"fornavn\"], key=\"ssb-common-key-1\")\n</code></pre> <p>Note that depseudonymization requires elevated access privileges.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome. To learn more, see the Contributor Guide.</p>"},{"location":"#license","title":"License","text":"<p>Distributed under the terms of the MIT license, Pseudonymization extensions for Dapla Toolbelt is free and open source software.</p>"},{"location":"#issues","title":"Issues","text":"<p>If you encounter any problems, please file an issue along with a detailed description.</p>"},{"location":"#credits","title":"Credits","text":"<p>This project was generated from @cjolowicz's Hypermodern Python Cookiecutter template.</p>"},{"location":"codeofconduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"codeofconduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"codeofconduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"codeofconduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"codeofconduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"codeofconduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dapla@ssb.no. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"codeofconduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"codeofconduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"codeofconduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"codeofconduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"codeofconduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"codeofconduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributor Guide","text":"<p>Thank you for your interest in improving this project. This project is open-source under the MIT license and welcomes contributions in the form of bug reports, feature requests, and pull requests.</p> <p>Here is a list of important resources for contributors:</p> <ul> <li>Source Code</li> <li>Documentation</li> <li>Issue Tracker</li> <li>Code of Conduct</li> </ul>"},{"location":"contributing/#how-to-report-a-bug","title":"How to report a bug","text":"<p>Report bugs on the Issue Tracker.</p> <p>When filing an issue, make sure to answer these questions:</p> <ul> <li>Which operating system and Python version are you using?</li> <li>Which version of this project are you using?</li> <li>What did you do?</li> <li>What did you expect to see?</li> <li>What did you see instead?</li> </ul> <p>The best way to get your bug fixed is to provide a test case, and/or steps to reproduce the issue.</p>"},{"location":"contributing/#how-to-request-a-feature","title":"How to request a feature","text":"<p>Request features on the Issue Tracker.</p>"},{"location":"contributing/#how-to-set-up-your-development-environment","title":"How to set up your development environment","text":"<p>You need Python 3.7+ and the following tools:</p> <ul> <li>Poetry</li> <li>Nox</li> <li>nox-poetry</li> </ul> <p>Install the package with development requirements:</p> <pre><code>$ poetry install\n</code></pre> <p>You can now run an interactive Python session, or the command-line interface:</p> <pre><code>$ poetry run python\n$ poetry run dapla-toolbelt-pseudo\n</code></pre>"},{"location":"contributing/#how-to-test-the-project","title":"How to test the project","text":"<p>Run the full test suite:</p> <pre><code>$ nox\n</code></pre> <p>List the available Nox sessions:</p> <pre><code>$ nox --list-sessions\n</code></pre> <p>You can also run a specific Nox session. For example, invoke the unit test suite like this:</p> <pre><code>$ nox --session=tests\n</code></pre> <p>Unit tests are located in the tests directory, and are written using the pytest testing framework.</p>"},{"location":"contributing/#local-testing","title":"Local Testing","text":"<p>When testing against a local instance of dapla-pseudo-service, you can configure the URL and authentication token by providing the following environment variables:</p> <pre><code>PSEUDO_SERVICE_URL=http://localhost:&lt;PORT&gt;\nPSEUDO_SERVICE_AUTH_TOKEN=&lt;KEYCLOAK_TOKEN&gt;\n</code></pre>"},{"location":"contributing/#how-to-submit-changes","title":"How to submit changes","text":"<p>Open a pull request to submit changes to this project.</p> <p>Your pull request needs to meet the following guidelines for acceptance:</p> <ul> <li>The Nox test suite must pass without errors and warnings.</li> <li>Include unit tests. This project maintains 100% code coverage.</li> <li>If your changes add functionality, update the documentation accordingly.</li> </ul> <p>Feel free to submit early, though\u2014we can always iterate on this.</p> <p>To run linting and code formatting checks before committing your change, you can install pre-commit as a Git hook by running the following command:</p> <pre><code>$ nox --session=pre-commit -- install\n</code></pre> <p>It is recommended to open an issue before starting work on anything. This will allow a chance to talk it over with the owners and validate your approach.</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#literalinclude-license","title":"```{literalinclude} ../LICENSE","text":""},{"location":"license/#language-none","title":"language: none","text":"<p>```</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#dapla-toolbelt-pseudo","title":"Dapla Toolbelt Pseudo","text":"<pre><code>.. automodule:: dapla_pseudo\n   :members:\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>dapla_pseudo<ul> <li>constants</li> <li>exceptions</li> <li>models</li> <li>types</li> <li>utils</li> <li>v1<ul> <li>builder_models</li> <li>builder_pseudo</li> <li>builder_validation</li> <li>client</li> <li>models</li> <li>ops</li> <li>supported_file_format</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/dapla_pseudo/","title":"dapla_pseudo","text":"<p>Pseudonymization extensions for Dapla Toolbelt.</p> <p>We import all methods from the v1 module as start. Later, if we need to support different versions, we can use this mechanism to define a default version. Users should typically not care about the underlying implementation, but they have the option to lock their implementation explicitly to a specific version if the need should arise, e.g. due to compatibility reasons.</p> <p>One should import functions like so: from dapla_pseudo import pseudonymize (which would resolve to the default implementation)</p> <p>Alternatively, lock the implementation to a specific version, like so: from dapla_pseudo.v1 import pseudonymize (which would always resolve to the v1 implementation)</p>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Field","title":"<code>Field</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>Field represents a targeted piece of data within a dataset or record.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>str</code> <p>field name or expression (e.g. a glob)</p> <code>mapping</code> <code>Optional[str]</code> <p>If defined, denotes a mapping transformation that should be applied before the operation in question, e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class Field(APIModel):\n    \"\"\"Field represents a targeted piece of data within a dataset or record.\n\n    Attributes:\n        pattern: field name or expression (e.g. a glob)\n        mapping: If defined, denotes a mapping transformation that should be applied before the operation in question,\n            e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.\n    \"\"\"\n\n    pattern: str\n    mapping: t.Optional[str] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient","title":"<code>PseudoClient</code>","text":"<p>Client for interacting with the Dapla Pseudo Service REST API.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>class PseudoClient:\n    \"\"\"Client for interacting with the Dapla Pseudo Service REST API.\"\"\"\n\n    PSEUDONYMIZE_FILE_ENDPOINT = \"pseudonymize/file\"\n\n    def __init__(\n        self,\n        pseudo_service_url: t.Optional[str] = None,\n        auth_token: t.Optional[str] = None,\n    ):\n        \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n        self.pseudo_service_url = (\n            \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n        )\n        self.static_auth_token = auth_token\n\n    def __auth_token(self) -&gt; str:\n        return str(AuthClient.fetch_personal_token()) if self.static_auth_token is None else str(self.static_auth_token)\n\n    def pseudonymize_file(\n        self,\n        pseudonymize_request: PseudonymizeFileRequest,\n        data: BinaryFileDecl,\n        timeout: int,\n        stream: bool = False,\n        name: t.Optional[str] = None,\n    ) -&gt; requests.Response:\n        \"\"\"Pseudonymize data from a file-like object.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a glob\n        (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n        :param pseudonymize_request: the request to send to Dapla Pseudo Service\n        :param data: file handle that should be pseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :param name: optional name for logging purposes\n        :return: pseudonymized data\n        \"\"\"\n        return self._post_to_file_endpoint(\n            self.PSEUDONYMIZE_FILE_ENDPOINT,\n            pseudonymize_request,\n            data,\n            self._extract_name(data, pseudonymize_request.target_content_type, name),\n            pseudonymize_request.target_content_type,\n            timeout,\n            stream,\n        )\n\n    def _extract_name(self, data: t.BinaryIO, content_type: Mimetypes, name: t.Optional[str]) -&gt; str:\n        if name is None:\n            try:\n                name = data.name\n            except AttributeError:\n                # Fallback to default name\n                name = \"unknown\"\n\n        if not name.endswith(\".json\") and content_type is Mimetypes.JSON:\n            name = f\"{name}.json\"  # Pseudo service expects a file extension\n\n        if \"/\" in name:\n            name = name.split(\"/\")[-1]  # Pseudo service expects a file name, not a path\n\n        return name\n\n    def depseudonymize_file(\n        self,\n        depseudonymize_request: DepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Notice that only certain whitelisted users can depseudonymize data.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: depseudonymized data\n        \"\"\"\n        return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n\n    def repseudonymize_file(\n        self,\n        repseudonymize_request: RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n        are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n        of rules are linked to separate keysets.\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: repseudonymized data\n        \"\"\"\n        return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n\n    def _process_file(\n        self,\n        operation: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        file_name = os.path.basename(file_path).split(\"/\")[-1]\n        content_type = Mimetypes(mimetypes.MimeTypes().guess_type(file_path)[0])\n\n        with open(file_path, \"rb\") as f:\n            return self._post_to_file_endpoint(\n                f\"{operation}/file\",\n                request,\n                f,\n                file_name,\n                content_type,\n                timeout,\n                stream,\n            )\n\n    def _post_to_file_endpoint(\n        self,\n        path: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        data: t.BinaryIO,\n        name: str,\n        content_type: Mimetypes,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        data_spec: FileSpecDecl = (name, data, content_type)\n        request_spec: FileSpecDecl = (None, request.to_json(), str(Mimetypes.JSON))\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Accept-Encoding\": \"gzip\",\n            },\n            files={\n                \"data\": data_spec,\n                \"request\": request_spec,\n            },\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_field_endpoint(\n        self,\n        path: str,\n        field_name: str,\n        values: list[str],\n        pseudo_func: t.Optional[PseudoFunction],\n        timeout: int,\n        keyset: t.Optional[PseudoKeyset] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\n            \"name\": field_name,\n            \"values\": values,\n            \"pseudoFunc\": str(pseudo_func),\n        }\n        if keyset:\n            request[\"keyset\"] = {\n                \"kekUri\": keyset.kek_uri,\n                \"encryptedKeyset\": keyset.encrypted_keyset,\n                \"keysetInfo\": keyset.keyset_info,\n            }\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Content-Type\": str(Mimetypes.JSON),\n            },\n            json=request,\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_sid_endpoint(\n        self,\n        path: str,\n        values: list[str],\n        sid_snapshot_date: t.Optional[str | date] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\"fnrList\": values}\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            params={\"snapshot\": str(sid_snapshot_date)} if sid_snapshot_date else None,\n            # Do not set content-type, as this will cause the json to serialize incorrectly\n            headers={\"Authorization\": f\"Bearer {self.__auth_token()}\"},\n            json=request,\n            stream=stream,\n            timeout=TIMEOUT_DEFAULT,  # seconds\n        )\n        response.raise_for_status()\n        return response\n\n    def export_dataset(self, request_json: str) -&gt; requests.Response:\n        \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n        The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n        It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n        This can be applied as a means to perform data minimization.\n\n        Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n        configuration and cannot be overridden.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :return: JSON response with a reference to the export \"job\"\n        \"\"\"\n        auth_token = self.__auth_token()\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/export\",\n            headers={\n                \"Authorization\": f\"Bearer {auth_token}\",\n                \"Content-Type\": \"application/json\",\n            },\n            data=request_json,\n            timeout=30,  # seconds\n        )\n        response.raise_for_status()\n        return response\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient.__init__","title":"<code>__init__(pseudo_service_url=None, auth_token=None)</code>","text":"<p>Use a default url for dapla-pseudo-service if not explicitly set.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def __init__(\n    self,\n    pseudo_service_url: t.Optional[str] = None,\n    auth_token: t.Optional[str] = None,\n):\n    \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n    self.pseudo_service_url = (\n        \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n    )\n    self.static_auth_token = auth_token\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient.depseudonymize_file","title":"<code>depseudonymize_file(depseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Notice that only certain whitelisted users can depseudonymize data.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def depseudonymize_file(\n    self,\n    depseudonymize_request: DepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Notice that only certain whitelisted users can depseudonymize data.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: depseudonymized data\n    \"\"\"\n    return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient.export_dataset","title":"<code>export_dataset(request_json)</code>","text":"<p>Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.</p> <p>The dataset will be archived in an encrypted zip file protected by a user provided password.</p> <p>It is possible to specify <code>columnSelectors</code>, that allows for partial export, e.g. only specific fields. This can be applied as a means to perform data minimization.</p> <p>Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application configuration and cannot be overridden.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :return: JSON response with a reference to the export \"job\"</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def export_dataset(self, request_json: str) -&gt; requests.Response:\n    \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n    The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n    It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n    This can be applied as a means to perform data minimization.\n\n    Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n    configuration and cannot be overridden.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :return: JSON response with a reference to the export \"job\"\n    \"\"\"\n    auth_token = self.__auth_token()\n    response = requests.post(\n        url=f\"{self.pseudo_service_url}/export\",\n        headers={\n            \"Authorization\": f\"Bearer {auth_token}\",\n            \"Content-Type\": \"application/json\",\n        },\n        data=request_json,\n        timeout=30,  # seconds\n    )\n    response.raise_for_status()\n    return response\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient.pseudonymize_file","title":"<code>pseudonymize_file(pseudonymize_request, data, timeout, stream=False, name=None)</code>","text":"<p>Pseudonymize data from a file-like object.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile</p> <p>:param pseudonymize_request: the request to send to Dapla Pseudo Service :param data: file handle that should be pseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :param name: optional name for logging purposes :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def pseudonymize_file(\n    self,\n    pseudonymize_request: PseudonymizeFileRequest,\n    data: BinaryFileDecl,\n    timeout: int,\n    stream: bool = False,\n    name: t.Optional[str] = None,\n) -&gt; requests.Response:\n    \"\"\"Pseudonymize data from a file-like object.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a glob\n    (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n    :param pseudonymize_request: the request to send to Dapla Pseudo Service\n    :param data: file handle that should be pseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :param name: optional name for logging purposes\n    :return: pseudonymized data\n    \"\"\"\n    return self._post_to_file_endpoint(\n        self.PSEUDONYMIZE_FILE_ENDPOINT,\n        pseudonymize_request,\n        data,\n        self._extract_name(data, pseudonymize_request.target_content_type, name),\n        pseudonymize_request.target_content_type,\n        timeout,\n        stream,\n    )\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoClient.repseudonymize_file","title":"<code>repseudonymize_file(repseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets of rules are linked to separate keysets.</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def repseudonymize_file(\n    self,\n    repseudonymize_request: RepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n    are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n    of rules are linked to separate keysets.\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: repseudonymized data\n    \"\"\"\n    return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData","title":"<code>PseudoData</code>","text":"<p>Starting point for pseudonymization of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>class PseudoData:\n    \"\"\"Starting point for pseudonymization of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    dataset: File | pl.DataFrame\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n        dataset: pl.DataFrame = pl.from_pandas(dataframe)\n        PseudoData.dataset = dataset\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n        PseudoData.dataset = dataframe\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n        Args:\n            dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified path.\n            FileInvalidError: If the file is empty.\n            ValueError: If the dataset is not of a supported type.\n            DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n        Returns:\n            _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import PseudoData\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            field_selector = PseudoData.from_file(bucket_path)\n\n            # Read from local filesystem\n            from dapla_pseudo import PseudoData\n\n            local_path = \"some_file.csv\"\n            field_selector = PseudoData.from_file(local_path))\n        \"\"\"\n        file_handle: t.Optional[BinaryFileDecl] = None\n        match dataset:\n            case str() | Path():\n                # File path\n                if str(dataset).startswith(\"gs://\"):\n                    try:\n                        file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                    except OSError as err:\n                        raise FileNotFoundError(\n                            f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                        ) from err\n                    except DefaultCredentialsError as err:\n                        raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n                else:\n                    file_handle = open(dataset, \"rb\")\n\n                file_handle.seek(0)\n\n            case io.BufferedReader():\n                # File handle\n                dataset.seek(0)\n                file_handle = dataset\n            case fsspec.spec.AbstractBufferedFile():\n                # This is a file handle to a remote storage system such as GCS.\n                # It provides random access for the underlying file-like data (without downloading the whole thing).\n                dataset.seek(0)\n                file_handle = io.BufferedReader(dataset)\n            case _:\n                raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n        if isinstance(file_handle, GCSFile):\n            file_size = file_handle.size\n        else:\n            file_size = os.fstat(file_handle.fileno()).st_size\n\n        if file_size == 0:\n            raise FileInvalidError(\"File is empty.\")\n\n        content_type = _get_content_type_from_file(file_handle)\n        PseudoData.dataset = File(file_handle, content_type)\n        return PseudoData._Pseudonymizer()\n\n    class _Pseudonymizer:\n        \"\"\"Select one or multiple fields to be pseudonymized.\"\"\"\n\n        def __init__(self, rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            \"\"\"Initialize the class.\"\"\"\n            self._rules: list[PseudoRule] = [] if rules is None else rules\n            self._pseudo_keyset: Optional[PseudoKeyset] = None\n            self._metadata: t.Dict[str, str] = {}\n            self._timeout: int = TIMEOUT_DEFAULT\n\n        def on_fields(self, *fields: str) -&gt; \"PseudoData._PseudoFuncSelector\":\n            \"\"\"Specify one or multiple fields to be pseudonymized.\"\"\"\n            return PseudoData._PseudoFuncSelector(list(fields), self._rules)\n\n        def pseudonymize(\n            self,\n            with_custom_keyset: Optional[PseudoKeyset] = None,\n            timeout: int = TIMEOUT_DEFAULT,\n        ) -&gt; Result:\n            \"\"\"Pseudonymize the dataset.\n\n            Args:\n                with_custom_keyset (PseudoKeyset, optional): The pseudonymization keyset to use. Defaults to None.\n                timeout (int): The timeout in seconds for the API call. Defaults to TIMEOUT_DEFAULT.\n\n            Raises:\n                ValueError: If no dataset has been provided, no fields have been provided, or the dataset is of an unsupported type.\n\n            Returns:\n                Result: The pseudonymized dataset and the associated metadata.\n            \"\"\"\n            if PseudoData.dataset is None:\n                raise ValueError(\"No dataset has been provided.\")\n\n            if self._rules == []:\n                raise ValueError(\"No fields have been provided. Use the 'on_fields' method.\")\n\n            if with_custom_keyset is not None:\n                self._pseudo_keyset = with_custom_keyset\n\n            self._timeout = timeout\n            match PseudoData.dataset:  # Differentiate between file and DataFrame\n                case File():\n                    return self._pseudonymize_file()\n                case pl.DataFrame():\n                    return self._pseudonymize_field()\n                case _:\n                    raise ValueError(\n                        f\"Unsupported data type: {type(PseudoData.dataset)}. Should only be DataFrame or file-like type.\"\n                    )\n\n        def _pseudonymize_file(self) -&gt; Result:\n            \"\"\"Pseudonymize the entire file.\"\"\"\n            # Need to type-cast explicitly. We know that PseudoData.dataset is a \"File\" if we reach this method.\n            file = t.cast(File, PseudoData.dataset)\n\n            pseudonymize_request = PseudonymizeFileRequest(\n                pseudo_config=PseudoConfig(\n                    rules=self._rules,\n                    keysets=KeyWrapper(self._pseudo_keyset).keyset_list(),\n                ),\n                target_content_type=file.content_type,\n                target_uri=None,\n                compression=None,\n            )\n\n            response: Response = _client().pseudonymize_file(\n                pseudonymize_request,\n                file.file_handle,\n                stream=True,\n                name=None,\n                timeout=self._timeout,\n            )\n            return Result(\n                PseudoFileResponse(response, file.content_type, streamed=True),\n                self._metadata,\n            )\n\n        def _pseudonymize_field(self) -&gt; Result:\n            \"\"\"Pseudonymizes the specified fields in the DataFrame using the provided pseudonymization function.\n\n            The pseudonymization is performed in parallel. After the parallel processing is finished,\n            the pseudonymized fields replace the original fields in the DataFrame stored in `self._dataframe`.\n\n            Returns:\n                Result: Containing the pseudonymized 'self._dataframe' and the associated metadata.\n            \"\"\"\n\n            def pseudonymize_field_runner(\n                field_name: str, series: pl.Series, pseudo_func: PseudoFunction\n            ) -&gt; tuple[str, pl.Series]:\n                \"\"\"Function that performs the pseudonymization on a pandas Series.\n\n                Args:\n                    field_name (str):  The name of the field.\n                    series (pl.Series): The pandas Series containing the values to be pseudonymized.\n                    pseudo_func (PseudoFunction): The pseudonymization function to apply to the values.\n\n                Returns:\n                    tuple[str,pl.Series]: A tuple containing the field_name and the corresponding series.\n                \"\"\"\n                return (\n                    field_name,\n                    _do_pseudonymize_field(\n                        path=\"pseudonymize/field\",\n                        field_name=field_name,\n                        values=series.to_list(),\n                        pseudo_func=pseudo_func,\n                        metadata_map=self._metadata,\n                        timeout=self._timeout,\n                        keyset=self._pseudo_keyset,\n                    ),\n                )\n\n            dataframe = t.cast(pd.DataFrame, PseudoData.dataset)\n            # Execute the pseudonymization API calls in parallel\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                pseudonymized_field: t.Dict[str, pl.Series] = {}\n                futures = [\n                    executor.submit(\n                        pseudonymize_field_runner,\n                        rule.pattern,\n                        dataframe[rule.pattern],\n                        rule.func,\n                    )\n                    for rule in self._rules\n                ]\n                # Wait for the futures to finish, then add each field to pseudonymized_field map\n                for future in concurrent.futures.as_completed(futures):\n                    result = future.result()\n                    # Each future result contains the field_name (0) and the pseudonymize_values (1)\n                    pseudonymized_field[result[0]] = result[1]\n\n                pseudonymized_df = pl.DataFrame(pseudonymized_field)\n                dataframe = dataframe.update(pseudonymized_df)\n\n            return Result(pseudo_response=dataframe, metadata=self._metadata)\n\n    class _PseudoFuncSelector:\n        def __init__(self, fields: list[str], rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            self._fields = fields\n            self._existing_rules = [] if rules is None else rules\n\n        def with_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; \"PseudoData._Pseudonymizer\":\n            \"\"\"Map selected fields to stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Self: The object configured to be mapped to stable ID\n            \"\"\"\n            function = PseudoFunction(\n                function_type=PseudoFunctionTypes.MAP_SID,\n                kwargs=MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)),\n            )\n            return self._rule_constructor(function)\n\n        def with_default_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.DAEAD, kwargs=DaeadKeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_papis_compatible_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.FF31, kwargs=FF31KeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_custom_function(self, function: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            return self._rule_constructor(function)\n\n        def _rule_constructor(self, func: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            # If we use the pseudonymize_file endpoint, we need a glob catch-all prefix.\n            rule_prefix = \"**/\" if isinstance(PseudoData.dataset, File) else \"\"\n            rules = [PseudoRule(name=None, func=func, pattern=f\"{rule_prefix}{field}\") for field in self._fields]\n            return PseudoData._Pseudonymizer(self._existing_rules + rules)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData.from_file","title":"<code>from_file(dataset)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>FileLikeDatasetDecl</code> <p>Either a path to the file to be read, or a file handle.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified path.</p> <code>FileInvalidError</code> <p>If the file is empty.</p> <code>ValueError</code> <p>If the dataset is not of a supported type.</p> <code>DefaultCredentialsError</code> <p>If no Google Authentication is found in the environment.</p> <p>Returns:</p> Name Type Description <code>_Pseudonymizer</code> <code>_Pseudonymizer</code> <p>An instance of the _Pseudonymizer class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import PseudoData bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" field_selector = PseudoData.from_file(bucket_path)</p>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import PseudoData</p> <p>local_path = \"some_file.csv\" field_selector = PseudoData.from_file(local_path))</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n    Args:\n        dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified path.\n        FileInvalidError: If the file is empty.\n        ValueError: If the dataset is not of a supported type.\n        DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n    Returns:\n        _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import PseudoData\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        field_selector = PseudoData.from_file(bucket_path)\n\n        # Read from local filesystem\n        from dapla_pseudo import PseudoData\n\n        local_path = \"some_file.csv\"\n        field_selector = PseudoData.from_file(local_path))\n    \"\"\"\n    file_handle: t.Optional[BinaryFileDecl] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            if str(dataset).startswith(\"gs://\"):\n                try:\n                    file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                except OSError as err:\n                    raise FileNotFoundError(\n                        f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                    ) from err\n                except DefaultCredentialsError as err:\n                    raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n            else:\n                file_handle = open(dataset, \"rb\")\n\n            file_handle.seek(0)\n\n        case io.BufferedReader():\n            # File handle\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n    if isinstance(file_handle, GCSFile):\n        file_size = file_handle.size\n    else:\n        file_size = os.fstat(file_handle.fileno()).st_size\n\n    if file_size == 0:\n        raise FileInvalidError(\"File is empty.\")\n\n    content_type = _get_content_type_from_file(file_handle)\n    PseudoData.dataset = File(file_handle, content_type)\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n    dataset: pl.DataFrame = pl.from_pandas(dataframe)\n    PseudoData.dataset = dataset\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.PseudoData.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n    PseudoData.dataset = dataframe\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator","title":"<code>Validator</code>","text":"<p>Starting point for validation of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>class Validator:\n    \"\"\"Starting point for validation of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n        Args:\n            file_path_str (str): The path to the file to be read.\n            kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified local path.\n\n        Returns:\n            _FieldSelector: An instance of the _FieldSelector class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import Validator\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n            field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n            # Read from local filesystem\n            from dapla_pseudo import Validator\n\n            local_path = \"some_file.csv\"\n            field_selector = Validator.from_file(local_path)\n        \"\"\"\n        file_path = Path(file_path_str)\n\n        if not file_path.is_file() and \"storage_options\" not in kwargs:\n            raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n        file_format = get_file_format_from_file_name(file_path)\n\n        return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n\n    class _FieldSelector:\n        \"\"\"Select a field to be validated.\"\"\"\n\n        def __init__(self, dataframe: pd.DataFrame | pl.DataFrame):\n            \"\"\"Initialize the class.\"\"\"\n            self._dataframe: pl.DataFrame\n            if isinstance(dataframe, pd.DataFrame):\n                self._dataframe = pl.from_pandas(dataframe)\n            else:\n                self._dataframe = dataframe\n\n        def on_field(self, field: str) -&gt; \"Validator._Validator\":\n            \"\"\"Specify a single field to be validated.\"\"\"\n            return Validator._Validator(self._dataframe, field)\n\n    class _Validator:\n        \"\"\"Assemble the validation request.\"\"\"\n\n        def __init__(\n            self,\n            dataframe: pl.DataFrame,\n            field: str,\n        ) -&gt; None:\n            self._dataframe: pl.DataFrame = dataframe\n            self._field: str = field\n\n        def validate_map_to_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; Result:\n            \"\"\"Checks if all the selected fields can be mapped to a stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Result: Containing a result dataframe with associated metadata.\n            \"\"\"\n            response: requests.Response = _client()._post_to_sid_endpoint(\n                \"sid/lookup/batch\",\n                self._dataframe[self._field].to_list(),\n                sid_snapshot_date,\n                stream=True,\n            )\n            # The response content is received as a buffered byte stream from the server.\n            # We decode the content using UTF-8, which gives us a List[Dict[str]] structure.\n            result_json = json.loads(response.content.decode(\"utf-8\"))[0]\n            result: Sequence[str] = []\n            metadata: dict[str, str] = {}\n            if \"missing\" in result_json:\n                result = result_json[\"missing\"]\n            if \"datasetExtractionSnapshotTime\" in result_json:\n                metadata = {\"datasetExtractionSnapshotTime\": result_json[\"datasetExtractionSnapshotTime\"]}\n\n            result_df = pl.DataFrame(pl.Series(self._field, result))\n            return Result(pseudo_response=result_df, metadata=metadata)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator.from_file","title":"<code>from_file(file_path_str, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path_str</code> <code>str</code> <p>The path to the file to be read.</p> required <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the file reader.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified local path.</p> <p>Returns:</p> Name Type Description <code>_FieldSelector</code> <code>_FieldSelector</code> <p>An instance of the _FieldSelector class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import Validator bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" storage_options = {\"token\": AuthClient.fetch_google_credentials()} field_selector = Validator.from_file(bucket_path, storage_options=storage_options)</p>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import Validator</p> <p>local_path = \"some_file.csv\" field_selector = Validator.from_file(local_path)</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n    Args:\n        file_path_str (str): The path to the file to be read.\n        kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified local path.\n\n    Returns:\n        _FieldSelector: An instance of the _FieldSelector class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import Validator\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n        field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n        # Read from local filesystem\n        from dapla_pseudo import Validator\n\n        local_path = \"some_file.csv\"\n        field_selector = Validator.from_file(local_path)\n    \"\"\"\n    file_path = Path(file_path_str)\n\n    if not file_path.is_file() and \"storage_options\" not in kwargs:\n        raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n    file_format = get_file_format_from_file_name(file_path)\n\n    return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.Validator.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.depseudonymize","title":"<code>depseudonymize(file_path, fields, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Depseudonymize specified fields of a local file.</p> <p>This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users) will be allowed to depseudonymize data.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer_deid.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json :param fields: list of fields that should be depseudonymized :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def depseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize specified fields of a local file.\n\n    This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users)\n    will be allowed to depseudonymize data.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer_deid.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json\n    :param fields: list of fields that should be depseudonymized\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: depseudonymized data\n    \"\"\"\n    content_type = mimetypes.MimeTypes().guess_type(file_path)[0]\n    k = KeyWrapper(key)\n    rules = _rules_of(fields=fields, sid_fields=[], key=k.key_id)\n    req = DepseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().depseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.pseudonymize","title":"<code>pseudonymize(dataset, fields=None, sid_fields=None, sid_snapshot_date=None, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Pseudonymize specified fields of a dataset.</p> The dataset may be supplied as <ul> <li>A local file on disk (string or Path)</li> <li>A file handle (io.BufferedReader)</li> <li>A Pandas dataframe</li> </ul> <p>Supported file formats: json, csv</p> <p>The <code>fields</code> and <code>sid_fields</code> lists specify what to pseudonymize. At least one of these fields must be specified. The list contents can either be plain field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply more advanced techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>For <code>fields</code>, the <code>daead</code> pseudonymization function is used. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>For <code>sid_fields</code>, the <code>map-sid</code> pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be compatible with those produced by the PAPIS project.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param dataset: path to file, file handle or dataframe :param fields: list of fields that should be pseudonymized :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def pseudonymize(\n    dataset: DatasetDecl,\n    fields: t.Optional[t.List[FieldDecl]] = None,\n    sid_fields: t.Optional[t.List[str]] = None,\n    sid_snapshot_date: t.Optional[str | date] = None,\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    r\"\"\"Pseudonymize specified fields of a dataset.\n\n    The dataset may be supplied as:\n        - A local file on disk (string or Path)\n        - A file handle (io.BufferedReader)\n        - A Pandas dataframe\n\n    Supported file formats: json, csv\n\n    The ``fields`` and ``sid_fields`` lists specify what to pseudonymize. At least one of these fields must be specified.\n    The list contents can either be plain field names (e.g. ``[\"some_field1\", \"another_field2\"]``, or you can apply\n    more advanced techniques, such as using wildcard characters (e.g. *Name) or slashes to target hierarchical fields\n    (e.g. **/path/to/hierarchicalStuff).\n\n    For ``fields``, the ``daead`` pseudonymization function is used. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    For ``sid_fields``, the ``map-sid`` pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and\n    subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be\n    compatible with those produced by the PAPIS project.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param dataset: path to file, file handle or dataframe\n    :param fields: list of fields that should be pseudonymized\n    :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized\n    :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: pseudonymized data\n    \"\"\"\n    if not fields and not sid_fields:\n        raise ValueError(\"At least one of fields and sid_fields must be specified.\")\n\n    # Avoid later type errors by making sure we have lists\n    if fields is None:\n        fields = []\n    if sid_fields is None:\n        sid_fields = []\n\n    file_handle: t.Optional[BinaryFileDecl] = None\n    name: t.Optional[str] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            content_type = Mimetypes(magic.from_file(dataset, mime=True))\n        case pd.DataFrame():\n            # Dataframe\n            content_type = Mimetypes.JSON\n            file_handle = _dataframe_to_json(dataset, fields, sid_fields)\n        case io.BufferedReader():\n            # File handle\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            name = dataset.path.split(\"/\")[-1] if hasattr(dataset, \"path\") else None\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {DatasetDecl}\")\n    k = KeyWrapper(key)\n    sid_func_kwargs = MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)) if sid_fields else None\n    rules = _rules_of(\n        fields=fields,\n        sid_fields=sid_fields or [],\n        key=k.key_id,\n        sid_func_kwargs=sid_func_kwargs,\n    )\n    pseudonymize_request = PseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    if file_handle is not None:\n        return _client().pseudonymize_file(pseudonymize_request, file_handle, stream=stream, name=name, timeout=timeout)\n    else:\n        return _client()._process_file(\n            \"pseudonymize\",\n            pseudonymize_request,\n            str(dataset),\n            stream=stream,\n            timeout=timeout,\n        )\n</code></pre>"},{"location":"reference/dapla_pseudo/#dapla_pseudo.repseudonymize","title":"<code>repseudonymize(file_path, fields, source_key=PredefinedKeys.SSB_COMMON_KEY_1, target_key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Repseudonymize specified fields of a local, previously pseudonymized file.</p> <p>You will need to provide a crypto key for both the source data and a key that should be used for re-pseudonymization.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json :param fields: list of fields that should be pseudonymized :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def repseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    source_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    target_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize specified fields of a local, previously pseudonymized file.\n\n    You will need to provide a crypto key for both the source data and a key that should be used for\n    re-pseudonymization.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json\n    :param fields: list of fields that should be pseudonymized\n    :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization\n    :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: repseudonymized data\n    \"\"\"\n    content_type = _content_type_of(file_path)\n    source_key_wrapper = KeyWrapper(source_key)\n    target_key_wrapper = KeyWrapper(target_key)\n    source_rules = _rules_of(fields=fields, sid_fields=[], key=source_key_wrapper.key_id)\n    target_rules = _rules_of(fields=fields, sid_fields=[], key=target_key_wrapper.key_id)\n    req = RepseudonymizeFileRequest(\n        source_pseudo_config=PseudoConfig(rules=source_rules, keysets=source_key_wrapper.keyset_list()),\n        target_pseudo_config=PseudoConfig(rules=target_rules, keysets=target_key_wrapper.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().repseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/","title":"constants","text":"<p>This module defines constants that are referenced throughout the codebase.</p>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.Env","title":"<code>Env</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Environment variable keys.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>class Env(str, Enum):\n    \"\"\"Environment variable keys.\"\"\"\n\n    PSEUDO_SERVICE_URL = \"PSEUDO_SERVICE_URL\"\n    PSEUDO_SERVICE_AUTH_TOKEN = \"PSEUDO_SERVICE_AUTH_TOKEN\"  # noqa S105\n\n    def __str__(self) -&gt; str:\n        \"\"\"Use value for string representation.\"\"\"\n        return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.Env.__str__","title":"<code>__str__()</code>","text":"<p>Use value for string representation.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Use value for string representation.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.PredefinedKeys","title":"<code>PredefinedKeys</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Names of 'global keys' that the Dapla Pseudo Service is familiar with.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>class PredefinedKeys(str, Enum):\n    \"\"\"Names of 'global keys' that the Dapla Pseudo Service is familiar with.\"\"\"\n\n    SSB_COMMON_KEY_1 = \"ssb-common-key-1\"\n    SSB_COMMON_KEY_2 = \"ssb-common-key-2\"\n    PAPIS_COMMON_KEY_1 = \"papis-common-key-1\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Use value for string representation.\"\"\"\n        return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.PredefinedKeys.__str__","title":"<code>__str__()</code>","text":"<p>Use value for string representation.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Use value for string representation.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.PseudoFunctionTypes","title":"<code>PseudoFunctionTypes</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Names of well known pseudo functions.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>class PseudoFunctionTypes(str, Enum):\n    \"\"\"Names of well known pseudo functions.\"\"\"\n\n    DAEAD = \"daead\"\n    MAP_SID = \"map-sid\"\n    FF31 = \"ff31\"\n    REDACT = \"redact\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Use value for string representation.\"\"\"\n        return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.PseudoFunctionTypes.__str__","title":"<code>__str__()</code>","text":"<p>Use value for string representation.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Use value for string representation.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.UnknownCharacterStrategy","title":"<code>UnknownCharacterStrategy</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>UnknownCharacterStrategy defines how encryption/decryption should handle non-alphabet characters.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>class UnknownCharacterStrategy(str, Enum):\n    \"\"\"UnknownCharacterStrategy defines how encryption/decryption should handle non-alphabet characters.\"\"\"\n\n    FAIL = \"fail\"\n    SKIP = \"skip\"\n    DELETE = \"delete\"\n    REDACT = \"redact\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Use value for string representation.\"\"\"\n        return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/constants/#dapla_pseudo.constants.UnknownCharacterStrategy.__str__","title":"<code>__str__()</code>","text":"<p>Use value for string representation.</p> Source code in <code>dapla_pseudo/constants.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Use value for string representation.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/","title":"exceptions","text":"<p>Common exceptions for the Dapla Pseudo package.</p>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.ExtensionNotValidError","title":"<code>ExtensionNotValidError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a file extension is invalid.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>class ExtensionNotValidError(Exception):\n    \"\"\"Exception raised when a file extension is invalid.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize the ExtensionNotValidError.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.ExtensionNotValidError.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize the ExtensionNotValidError.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize the ExtensionNotValidError.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.FileInvalidError","title":"<code>FileInvalidError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a file is in an invalid state.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>class FileInvalidError(Exception):\n    \"\"\"Exception raised when a file is in an invalid state.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize the NoFileExtensionError.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.FileInvalidError.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize the NoFileExtensionError.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize the NoFileExtensionError.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.MimetypeNotSupportedError","title":"<code>MimetypeNotSupportedError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a Mimetype is invalid.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>class MimetypeNotSupportedError(Exception):\n    \"\"\"Exception raised when a Mimetype is invalid.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize the ExtensionNotValidError.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.MimetypeNotSupportedError.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize the ExtensionNotValidError.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize the ExtensionNotValidError.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.NoFileExtensionError","title":"<code>NoFileExtensionError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when a file has no file extension.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>class NoFileExtensionError(Exception):\n    \"\"\"Exception raised when a file has no file extension.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize the NoFileExtensionError.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/exceptions/#dapla_pseudo.exceptions.NoFileExtensionError.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize the NoFileExtensionError.</p> Source code in <code>dapla_pseudo/exceptions.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize the NoFileExtensionError.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dapla_pseudo/models/","title":"models","text":"<p>The models module contains base classes used by other models.</p>"},{"location":"reference/dapla_pseudo/models/#dapla_pseudo.models.APIModel","title":"<code>APIModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>APIModel is a base class for models that are used for communicating with the Dapla Pseudo Service.</p> <p>It provides configuration for serializing/converting between camelCase (required by the API) and snake_case (used pythonically by this lib). It also provides some good defaults for converting a model to JSON.</p> Source code in <code>dapla_pseudo/models.py</code> <pre><code>class APIModel(BaseModel):\n    \"\"\"APIModel is a base class for models that are used for communicating with the Dapla Pseudo Service.\n\n    It provides configuration for serializing/converting between camelCase (required by the API) and\n    snake_case (used pythonically by this lib). It also provides some good defaults for converting a\n    model to JSON.\n    \"\"\"\n\n    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True)\n\n    def to_json(self) -&gt; str:\n        \"\"\"Convert the model to JSON using camelCase aliases and only including assigned values.\"\"\"\n        return self.model_dump_json(exclude_unset=True, exclude_none=True, by_alias=True)\n</code></pre>"},{"location":"reference/dapla_pseudo/models/#dapla_pseudo.models.APIModel.to_json","title":"<code>to_json()</code>","text":"<p>Convert the model to JSON using camelCase aliases and only including assigned values.</p> Source code in <code>dapla_pseudo/models.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert the model to JSON using camelCase aliases and only including assigned values.\"\"\"\n    return self.model_dump_json(exclude_unset=True, exclude_none=True, by_alias=True)\n</code></pre>"},{"location":"reference/dapla_pseudo/types/","title":"types","text":"<p>Type declarations for dapla-toolbelt-pseudo.</p>"},{"location":"reference/dapla_pseudo/utils/","title":"utils","text":"<p>Utility functions for Dapla Pseudo.</p>"},{"location":"reference/dapla_pseudo/utils/#dapla_pseudo.utils.convert_to_date","title":"<code>convert_to_date(sid_snapshot_date)</code>","text":"<p>Converts the SID version date to the 'date' type.</p> Source code in <code>dapla_pseudo/utils.py</code> <pre><code>def convert_to_date(sid_snapshot_date: t.Optional[date | str]) -&gt; t.Optional[date]:\n    \"\"\"Converts the SID version date to the 'date' type.\"\"\"\n    if isinstance(sid_snapshot_date, str):\n        try:\n            return date.fromisoformat(sid_snapshot_date)\n        except ValueError as exc:\n            raise ValueError(\"Version timestamp must be a valid ISO date string (YYYY-MM-DD)\") from exc\n    return sid_snapshot_date\n</code></pre>"},{"location":"reference/dapla_pseudo/utils/#dapla_pseudo.utils.find_multipart_obj","title":"<code>find_multipart_obj(obj_name, multipart_files_tuple)</code>","text":"<p>Find \"multipart object\" by name.</p> <p>The requests lib specifies multipart file arguments as file-tuples, such as ('filename', fileobj, 'content_type') This method searches a tuple of such file-tuples ((file-tuple1),...,(file-tupleN)) It returns the fileobj for the first matching file-tuple with a specified filename.</p> <p>Example: Given the multipart_files_tuple: multipart_tuple = (('filename1', fileobj1, 'application/json'), ('filename2', fileobj2, 'application/json'))</p> <p>then find_multipart_obj(\"filename2\", multipart_tuple) -&gt; fileobj</p> Source code in <code>dapla_pseudo/utils.py</code> <pre><code>def find_multipart_obj(obj_name: str, multipart_files_tuple: t.Set[t.Any]) -&gt; t.Any:\n    \"\"\"Find \"multipart object\" by name.\n\n    The requests lib specifies multipart file arguments as file-tuples, such as\n    ('filename', fileobj, 'content_type')\n    This method searches a tuple of such file-tuples ((file-tuple1),...,(file-tupleN))\n    It returns the fileobj for the first matching file-tuple with a specified filename.\n\n    Example:\n    Given the multipart_files_tuple:\n    multipart_tuple = (('filename1', fileobj1, 'application/json'), ('filename2', fileobj2, 'application/json'))\n\n    then\n    find_multipart_obj(\"filename2\", multipart_tuple) -&gt; fileobj\n    \"\"\"\n    try:\n        matching_item = next(item[1] for item in multipart_files_tuple if item[0] == obj_name)\n        return matching_item[1]\n    except StopIteration:\n        return None\n</code></pre>"},{"location":"reference/dapla_pseudo/utils/#dapla_pseudo.utils.get_file_format_from_file_name","title":"<code>get_file_format_from_file_name(file_path)</code>","text":"<p>Extracts the file format from a file path.</p> Source code in <code>dapla_pseudo/utils.py</code> <pre><code>def get_file_format_from_file_name(file_path: str | Path) -&gt; SupportedOutputFileFormat:\n    \"\"\"Extracts the file format from a file path.\"\"\"\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    file_extension = file_path.suffix\n    if not file_extension:\n        raise NoFileExtensionError(f\"File path '{file_path}' has no file extension.\")\n    file_format = SupportedOutputFileFormat(file_extension.replace(\".\", \"\"))\n    return file_format\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/","title":"v1","text":"<p>Pseudo Service version 1 implementation.</p>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Field","title":"<code>Field</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>Field represents a targeted piece of data within a dataset or record.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>str</code> <p>field name or expression (e.g. a glob)</p> <code>mapping</code> <code>Optional[str]</code> <p>If defined, denotes a mapping transformation that should be applied before the operation in question, e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class Field(APIModel):\n    \"\"\"Field represents a targeted piece of data within a dataset or record.\n\n    Attributes:\n        pattern: field name or expression (e.g. a glob)\n        mapping: If defined, denotes a mapping transformation that should be applied before the operation in question,\n            e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.\n    \"\"\"\n\n    pattern: str\n    mapping: t.Optional[str] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient","title":"<code>PseudoClient</code>","text":"<p>Client for interacting with the Dapla Pseudo Service REST API.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>class PseudoClient:\n    \"\"\"Client for interacting with the Dapla Pseudo Service REST API.\"\"\"\n\n    PSEUDONYMIZE_FILE_ENDPOINT = \"pseudonymize/file\"\n\n    def __init__(\n        self,\n        pseudo_service_url: t.Optional[str] = None,\n        auth_token: t.Optional[str] = None,\n    ):\n        \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n        self.pseudo_service_url = (\n            \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n        )\n        self.static_auth_token = auth_token\n\n    def __auth_token(self) -&gt; str:\n        return str(AuthClient.fetch_personal_token()) if self.static_auth_token is None else str(self.static_auth_token)\n\n    def pseudonymize_file(\n        self,\n        pseudonymize_request: PseudonymizeFileRequest,\n        data: BinaryFileDecl,\n        timeout: int,\n        stream: bool = False,\n        name: t.Optional[str] = None,\n    ) -&gt; requests.Response:\n        \"\"\"Pseudonymize data from a file-like object.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a glob\n        (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n        :param pseudonymize_request: the request to send to Dapla Pseudo Service\n        :param data: file handle that should be pseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :param name: optional name for logging purposes\n        :return: pseudonymized data\n        \"\"\"\n        return self._post_to_file_endpoint(\n            self.PSEUDONYMIZE_FILE_ENDPOINT,\n            pseudonymize_request,\n            data,\n            self._extract_name(data, pseudonymize_request.target_content_type, name),\n            pseudonymize_request.target_content_type,\n            timeout,\n            stream,\n        )\n\n    def _extract_name(self, data: t.BinaryIO, content_type: Mimetypes, name: t.Optional[str]) -&gt; str:\n        if name is None:\n            try:\n                name = data.name\n            except AttributeError:\n                # Fallback to default name\n                name = \"unknown\"\n\n        if not name.endswith(\".json\") and content_type is Mimetypes.JSON:\n            name = f\"{name}.json\"  # Pseudo service expects a file extension\n\n        if \"/\" in name:\n            name = name.split(\"/\")[-1]  # Pseudo service expects a file name, not a path\n\n        return name\n\n    def depseudonymize_file(\n        self,\n        depseudonymize_request: DepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Notice that only certain whitelisted users can depseudonymize data.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: depseudonymized data\n        \"\"\"\n        return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n\n    def repseudonymize_file(\n        self,\n        repseudonymize_request: RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n        are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n        of rules are linked to separate keysets.\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: repseudonymized data\n        \"\"\"\n        return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n\n    def _process_file(\n        self,\n        operation: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        file_name = os.path.basename(file_path).split(\"/\")[-1]\n        content_type = Mimetypes(mimetypes.MimeTypes().guess_type(file_path)[0])\n\n        with open(file_path, \"rb\") as f:\n            return self._post_to_file_endpoint(\n                f\"{operation}/file\",\n                request,\n                f,\n                file_name,\n                content_type,\n                timeout,\n                stream,\n            )\n\n    def _post_to_file_endpoint(\n        self,\n        path: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        data: t.BinaryIO,\n        name: str,\n        content_type: Mimetypes,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        data_spec: FileSpecDecl = (name, data, content_type)\n        request_spec: FileSpecDecl = (None, request.to_json(), str(Mimetypes.JSON))\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Accept-Encoding\": \"gzip\",\n            },\n            files={\n                \"data\": data_spec,\n                \"request\": request_spec,\n            },\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_field_endpoint(\n        self,\n        path: str,\n        field_name: str,\n        values: list[str],\n        pseudo_func: t.Optional[PseudoFunction],\n        timeout: int,\n        keyset: t.Optional[PseudoKeyset] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\n            \"name\": field_name,\n            \"values\": values,\n            \"pseudoFunc\": str(pseudo_func),\n        }\n        if keyset:\n            request[\"keyset\"] = {\n                \"kekUri\": keyset.kek_uri,\n                \"encryptedKeyset\": keyset.encrypted_keyset,\n                \"keysetInfo\": keyset.keyset_info,\n            }\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Content-Type\": str(Mimetypes.JSON),\n            },\n            json=request,\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_sid_endpoint(\n        self,\n        path: str,\n        values: list[str],\n        sid_snapshot_date: t.Optional[str | date] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\"fnrList\": values}\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            params={\"snapshot\": str(sid_snapshot_date)} if sid_snapshot_date else None,\n            # Do not set content-type, as this will cause the json to serialize incorrectly\n            headers={\"Authorization\": f\"Bearer {self.__auth_token()}\"},\n            json=request,\n            stream=stream,\n            timeout=TIMEOUT_DEFAULT,  # seconds\n        )\n        response.raise_for_status()\n        return response\n\n    def export_dataset(self, request_json: str) -&gt; requests.Response:\n        \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n        The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n        It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n        This can be applied as a means to perform data minimization.\n\n        Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n        configuration and cannot be overridden.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :return: JSON response with a reference to the export \"job\"\n        \"\"\"\n        auth_token = self.__auth_token()\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/export\",\n            headers={\n                \"Authorization\": f\"Bearer {auth_token}\",\n                \"Content-Type\": \"application/json\",\n            },\n            data=request_json,\n            timeout=30,  # seconds\n        )\n        response.raise_for_status()\n        return response\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient.__init__","title":"<code>__init__(pseudo_service_url=None, auth_token=None)</code>","text":"<p>Use a default url for dapla-pseudo-service if not explicitly set.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def __init__(\n    self,\n    pseudo_service_url: t.Optional[str] = None,\n    auth_token: t.Optional[str] = None,\n):\n    \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n    self.pseudo_service_url = (\n        \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n    )\n    self.static_auth_token = auth_token\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient.depseudonymize_file","title":"<code>depseudonymize_file(depseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Notice that only certain whitelisted users can depseudonymize data.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def depseudonymize_file(\n    self,\n    depseudonymize_request: DepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Notice that only certain whitelisted users can depseudonymize data.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: depseudonymized data\n    \"\"\"\n    return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient.export_dataset","title":"<code>export_dataset(request_json)</code>","text":"<p>Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.</p> <p>The dataset will be archived in an encrypted zip file protected by a user provided password.</p> <p>It is possible to specify <code>columnSelectors</code>, that allows for partial export, e.g. only specific fields. This can be applied as a means to perform data minimization.</p> <p>Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application configuration and cannot be overridden.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :return: JSON response with a reference to the export \"job\"</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def export_dataset(self, request_json: str) -&gt; requests.Response:\n    \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n    The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n    It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n    This can be applied as a means to perform data minimization.\n\n    Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n    configuration and cannot be overridden.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :return: JSON response with a reference to the export \"job\"\n    \"\"\"\n    auth_token = self.__auth_token()\n    response = requests.post(\n        url=f\"{self.pseudo_service_url}/export\",\n        headers={\n            \"Authorization\": f\"Bearer {auth_token}\",\n            \"Content-Type\": \"application/json\",\n        },\n        data=request_json,\n        timeout=30,  # seconds\n    )\n    response.raise_for_status()\n    return response\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient.pseudonymize_file","title":"<code>pseudonymize_file(pseudonymize_request, data, timeout, stream=False, name=None)</code>","text":"<p>Pseudonymize data from a file-like object.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile</p> <p>:param pseudonymize_request: the request to send to Dapla Pseudo Service :param data: file handle that should be pseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :param name: optional name for logging purposes :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def pseudonymize_file(\n    self,\n    pseudonymize_request: PseudonymizeFileRequest,\n    data: BinaryFileDecl,\n    timeout: int,\n    stream: bool = False,\n    name: t.Optional[str] = None,\n) -&gt; requests.Response:\n    \"\"\"Pseudonymize data from a file-like object.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a glob\n    (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n    :param pseudonymize_request: the request to send to Dapla Pseudo Service\n    :param data: file handle that should be pseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :param name: optional name for logging purposes\n    :return: pseudonymized data\n    \"\"\"\n    return self._post_to_file_endpoint(\n        self.PSEUDONYMIZE_FILE_ENDPOINT,\n        pseudonymize_request,\n        data,\n        self._extract_name(data, pseudonymize_request.target_content_type, name),\n        pseudonymize_request.target_content_type,\n        timeout,\n        stream,\n    )\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoClient.repseudonymize_file","title":"<code>repseudonymize_file(repseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets of rules are linked to separate keysets.</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def repseudonymize_file(\n    self,\n    repseudonymize_request: RepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n    are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n    of rules are linked to separate keysets.\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: repseudonymized data\n    \"\"\"\n    return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData","title":"<code>PseudoData</code>","text":"<p>Starting point for pseudonymization of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>class PseudoData:\n    \"\"\"Starting point for pseudonymization of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    dataset: File | pl.DataFrame\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n        dataset: pl.DataFrame = pl.from_pandas(dataframe)\n        PseudoData.dataset = dataset\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n        PseudoData.dataset = dataframe\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n        Args:\n            dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified path.\n            FileInvalidError: If the file is empty.\n            ValueError: If the dataset is not of a supported type.\n            DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n        Returns:\n            _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import PseudoData\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            field_selector = PseudoData.from_file(bucket_path)\n\n            # Read from local filesystem\n            from dapla_pseudo import PseudoData\n\n            local_path = \"some_file.csv\"\n            field_selector = PseudoData.from_file(local_path))\n        \"\"\"\n        file_handle: t.Optional[BinaryFileDecl] = None\n        match dataset:\n            case str() | Path():\n                # File path\n                if str(dataset).startswith(\"gs://\"):\n                    try:\n                        file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                    except OSError as err:\n                        raise FileNotFoundError(\n                            f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                        ) from err\n                    except DefaultCredentialsError as err:\n                        raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n                else:\n                    file_handle = open(dataset, \"rb\")\n\n                file_handle.seek(0)\n\n            case io.BufferedReader():\n                # File handle\n                dataset.seek(0)\n                file_handle = dataset\n            case fsspec.spec.AbstractBufferedFile():\n                # This is a file handle to a remote storage system such as GCS.\n                # It provides random access for the underlying file-like data (without downloading the whole thing).\n                dataset.seek(0)\n                file_handle = io.BufferedReader(dataset)\n            case _:\n                raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n        if isinstance(file_handle, GCSFile):\n            file_size = file_handle.size\n        else:\n            file_size = os.fstat(file_handle.fileno()).st_size\n\n        if file_size == 0:\n            raise FileInvalidError(\"File is empty.\")\n\n        content_type = _get_content_type_from_file(file_handle)\n        PseudoData.dataset = File(file_handle, content_type)\n        return PseudoData._Pseudonymizer()\n\n    class _Pseudonymizer:\n        \"\"\"Select one or multiple fields to be pseudonymized.\"\"\"\n\n        def __init__(self, rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            \"\"\"Initialize the class.\"\"\"\n            self._rules: list[PseudoRule] = [] if rules is None else rules\n            self._pseudo_keyset: Optional[PseudoKeyset] = None\n            self._metadata: t.Dict[str, str] = {}\n            self._timeout: int = TIMEOUT_DEFAULT\n\n        def on_fields(self, *fields: str) -&gt; \"PseudoData._PseudoFuncSelector\":\n            \"\"\"Specify one or multiple fields to be pseudonymized.\"\"\"\n            return PseudoData._PseudoFuncSelector(list(fields), self._rules)\n\n        def pseudonymize(\n            self,\n            with_custom_keyset: Optional[PseudoKeyset] = None,\n            timeout: int = TIMEOUT_DEFAULT,\n        ) -&gt; Result:\n            \"\"\"Pseudonymize the dataset.\n\n            Args:\n                with_custom_keyset (PseudoKeyset, optional): The pseudonymization keyset to use. Defaults to None.\n                timeout (int): The timeout in seconds for the API call. Defaults to TIMEOUT_DEFAULT.\n\n            Raises:\n                ValueError: If no dataset has been provided, no fields have been provided, or the dataset is of an unsupported type.\n\n            Returns:\n                Result: The pseudonymized dataset and the associated metadata.\n            \"\"\"\n            if PseudoData.dataset is None:\n                raise ValueError(\"No dataset has been provided.\")\n\n            if self._rules == []:\n                raise ValueError(\"No fields have been provided. Use the 'on_fields' method.\")\n\n            if with_custom_keyset is not None:\n                self._pseudo_keyset = with_custom_keyset\n\n            self._timeout = timeout\n            match PseudoData.dataset:  # Differentiate between file and DataFrame\n                case File():\n                    return self._pseudonymize_file()\n                case pl.DataFrame():\n                    return self._pseudonymize_field()\n                case _:\n                    raise ValueError(\n                        f\"Unsupported data type: {type(PseudoData.dataset)}. Should only be DataFrame or file-like type.\"\n                    )\n\n        def _pseudonymize_file(self) -&gt; Result:\n            \"\"\"Pseudonymize the entire file.\"\"\"\n            # Need to type-cast explicitly. We know that PseudoData.dataset is a \"File\" if we reach this method.\n            file = t.cast(File, PseudoData.dataset)\n\n            pseudonymize_request = PseudonymizeFileRequest(\n                pseudo_config=PseudoConfig(\n                    rules=self._rules,\n                    keysets=KeyWrapper(self._pseudo_keyset).keyset_list(),\n                ),\n                target_content_type=file.content_type,\n                target_uri=None,\n                compression=None,\n            )\n\n            response: Response = _client().pseudonymize_file(\n                pseudonymize_request,\n                file.file_handle,\n                stream=True,\n                name=None,\n                timeout=self._timeout,\n            )\n            return Result(\n                PseudoFileResponse(response, file.content_type, streamed=True),\n                self._metadata,\n            )\n\n        def _pseudonymize_field(self) -&gt; Result:\n            \"\"\"Pseudonymizes the specified fields in the DataFrame using the provided pseudonymization function.\n\n            The pseudonymization is performed in parallel. After the parallel processing is finished,\n            the pseudonymized fields replace the original fields in the DataFrame stored in `self._dataframe`.\n\n            Returns:\n                Result: Containing the pseudonymized 'self._dataframe' and the associated metadata.\n            \"\"\"\n\n            def pseudonymize_field_runner(\n                field_name: str, series: pl.Series, pseudo_func: PseudoFunction\n            ) -&gt; tuple[str, pl.Series]:\n                \"\"\"Function that performs the pseudonymization on a pandas Series.\n\n                Args:\n                    field_name (str):  The name of the field.\n                    series (pl.Series): The pandas Series containing the values to be pseudonymized.\n                    pseudo_func (PseudoFunction): The pseudonymization function to apply to the values.\n\n                Returns:\n                    tuple[str,pl.Series]: A tuple containing the field_name and the corresponding series.\n                \"\"\"\n                return (\n                    field_name,\n                    _do_pseudonymize_field(\n                        path=\"pseudonymize/field\",\n                        field_name=field_name,\n                        values=series.to_list(),\n                        pseudo_func=pseudo_func,\n                        metadata_map=self._metadata,\n                        timeout=self._timeout,\n                        keyset=self._pseudo_keyset,\n                    ),\n                )\n\n            dataframe = t.cast(pd.DataFrame, PseudoData.dataset)\n            # Execute the pseudonymization API calls in parallel\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                pseudonymized_field: t.Dict[str, pl.Series] = {}\n                futures = [\n                    executor.submit(\n                        pseudonymize_field_runner,\n                        rule.pattern,\n                        dataframe[rule.pattern],\n                        rule.func,\n                    )\n                    for rule in self._rules\n                ]\n                # Wait for the futures to finish, then add each field to pseudonymized_field map\n                for future in concurrent.futures.as_completed(futures):\n                    result = future.result()\n                    # Each future result contains the field_name (0) and the pseudonymize_values (1)\n                    pseudonymized_field[result[0]] = result[1]\n\n                pseudonymized_df = pl.DataFrame(pseudonymized_field)\n                dataframe = dataframe.update(pseudonymized_df)\n\n            return Result(pseudo_response=dataframe, metadata=self._metadata)\n\n    class _PseudoFuncSelector:\n        def __init__(self, fields: list[str], rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            self._fields = fields\n            self._existing_rules = [] if rules is None else rules\n\n        def with_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; \"PseudoData._Pseudonymizer\":\n            \"\"\"Map selected fields to stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Self: The object configured to be mapped to stable ID\n            \"\"\"\n            function = PseudoFunction(\n                function_type=PseudoFunctionTypes.MAP_SID,\n                kwargs=MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)),\n            )\n            return self._rule_constructor(function)\n\n        def with_default_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.DAEAD, kwargs=DaeadKeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_papis_compatible_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.FF31, kwargs=FF31KeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_custom_function(self, function: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            return self._rule_constructor(function)\n\n        def _rule_constructor(self, func: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            # If we use the pseudonymize_file endpoint, we need a glob catch-all prefix.\n            rule_prefix = \"**/\" if isinstance(PseudoData.dataset, File) else \"\"\n            rules = [PseudoRule(name=None, func=func, pattern=f\"{rule_prefix}{field}\") for field in self._fields]\n            return PseudoData._Pseudonymizer(self._existing_rules + rules)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData.from_file","title":"<code>from_file(dataset)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>FileLikeDatasetDecl</code> <p>Either a path to the file to be read, or a file handle.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified path.</p> <code>FileInvalidError</code> <p>If the file is empty.</p> <code>ValueError</code> <p>If the dataset is not of a supported type.</p> <code>DefaultCredentialsError</code> <p>If no Google Authentication is found in the environment.</p> <p>Returns:</p> Name Type Description <code>_Pseudonymizer</code> <code>_Pseudonymizer</code> <p>An instance of the _Pseudonymizer class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import PseudoData bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" field_selector = PseudoData.from_file(bucket_path)</p>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import PseudoData</p> <p>local_path = \"some_file.csv\" field_selector = PseudoData.from_file(local_path))</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n    Args:\n        dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified path.\n        FileInvalidError: If the file is empty.\n        ValueError: If the dataset is not of a supported type.\n        DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n    Returns:\n        _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import PseudoData\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        field_selector = PseudoData.from_file(bucket_path)\n\n        # Read from local filesystem\n        from dapla_pseudo import PseudoData\n\n        local_path = \"some_file.csv\"\n        field_selector = PseudoData.from_file(local_path))\n    \"\"\"\n    file_handle: t.Optional[BinaryFileDecl] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            if str(dataset).startswith(\"gs://\"):\n                try:\n                    file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                except OSError as err:\n                    raise FileNotFoundError(\n                        f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                    ) from err\n                except DefaultCredentialsError as err:\n                    raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n            else:\n                file_handle = open(dataset, \"rb\")\n\n            file_handle.seek(0)\n\n        case io.BufferedReader():\n            # File handle\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n    if isinstance(file_handle, GCSFile):\n        file_size = file_handle.size\n    else:\n        file_size = os.fstat(file_handle.fileno()).st_size\n\n    if file_size == 0:\n        raise FileInvalidError(\"File is empty.\")\n\n    content_type = _get_content_type_from_file(file_handle)\n    PseudoData.dataset = File(file_handle, content_type)\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n    dataset: pl.DataFrame = pl.from_pandas(dataframe)\n    PseudoData.dataset = dataset\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.PseudoData.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n    PseudoData.dataset = dataframe\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator","title":"<code>Validator</code>","text":"<p>Starting point for validation of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>class Validator:\n    \"\"\"Starting point for validation of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n        Args:\n            file_path_str (str): The path to the file to be read.\n            kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified local path.\n\n        Returns:\n            _FieldSelector: An instance of the _FieldSelector class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import Validator\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n            field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n            # Read from local filesystem\n            from dapla_pseudo import Validator\n\n            local_path = \"some_file.csv\"\n            field_selector = Validator.from_file(local_path)\n        \"\"\"\n        file_path = Path(file_path_str)\n\n        if not file_path.is_file() and \"storage_options\" not in kwargs:\n            raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n        file_format = get_file_format_from_file_name(file_path)\n\n        return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n\n    class _FieldSelector:\n        \"\"\"Select a field to be validated.\"\"\"\n\n        def __init__(self, dataframe: pd.DataFrame | pl.DataFrame):\n            \"\"\"Initialize the class.\"\"\"\n            self._dataframe: pl.DataFrame\n            if isinstance(dataframe, pd.DataFrame):\n                self._dataframe = pl.from_pandas(dataframe)\n            else:\n                self._dataframe = dataframe\n\n        def on_field(self, field: str) -&gt; \"Validator._Validator\":\n            \"\"\"Specify a single field to be validated.\"\"\"\n            return Validator._Validator(self._dataframe, field)\n\n    class _Validator:\n        \"\"\"Assemble the validation request.\"\"\"\n\n        def __init__(\n            self,\n            dataframe: pl.DataFrame,\n            field: str,\n        ) -&gt; None:\n            self._dataframe: pl.DataFrame = dataframe\n            self._field: str = field\n\n        def validate_map_to_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; Result:\n            \"\"\"Checks if all the selected fields can be mapped to a stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Result: Containing a result dataframe with associated metadata.\n            \"\"\"\n            response: requests.Response = _client()._post_to_sid_endpoint(\n                \"sid/lookup/batch\",\n                self._dataframe[self._field].to_list(),\n                sid_snapshot_date,\n                stream=True,\n            )\n            # The response content is received as a buffered byte stream from the server.\n            # We decode the content using UTF-8, which gives us a List[Dict[str]] structure.\n            result_json = json.loads(response.content.decode(\"utf-8\"))[0]\n            result: Sequence[str] = []\n            metadata: dict[str, str] = {}\n            if \"missing\" in result_json:\n                result = result_json[\"missing\"]\n            if \"datasetExtractionSnapshotTime\" in result_json:\n                metadata = {\"datasetExtractionSnapshotTime\": result_json[\"datasetExtractionSnapshotTime\"]}\n\n            result_df = pl.DataFrame(pl.Series(self._field, result))\n            return Result(pseudo_response=result_df, metadata=metadata)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator.from_file","title":"<code>from_file(file_path_str, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path_str</code> <code>str</code> <p>The path to the file to be read.</p> required <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the file reader.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified local path.</p> <p>Returns:</p> Name Type Description <code>_FieldSelector</code> <code>_FieldSelector</code> <p>An instance of the _FieldSelector class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import Validator bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" storage_options = {\"token\": AuthClient.fetch_google_credentials()} field_selector = Validator.from_file(bucket_path, storage_options=storage_options)</p>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import Validator</p> <p>local_path = \"some_file.csv\" field_selector = Validator.from_file(local_path)</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n    Args:\n        file_path_str (str): The path to the file to be read.\n        kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified local path.\n\n    Returns:\n        _FieldSelector: An instance of the _FieldSelector class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import Validator\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n        field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n        # Read from local filesystem\n        from dapla_pseudo import Validator\n\n        local_path = \"some_file.csv\"\n        field_selector = Validator.from_file(local_path)\n    \"\"\"\n    file_path = Path(file_path_str)\n\n    if not file_path.is_file() and \"storage_options\" not in kwargs:\n        raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n    file_format = get_file_format_from_file_name(file_path)\n\n    return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.Validator.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.depseudonymize","title":"<code>depseudonymize(file_path, fields, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Depseudonymize specified fields of a local file.</p> <p>This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users) will be allowed to depseudonymize data.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer_deid.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json :param fields: list of fields that should be depseudonymized :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def depseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize specified fields of a local file.\n\n    This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users)\n    will be allowed to depseudonymize data.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer_deid.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json\n    :param fields: list of fields that should be depseudonymized\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: depseudonymized data\n    \"\"\"\n    content_type = mimetypes.MimeTypes().guess_type(file_path)[0]\n    k = KeyWrapper(key)\n    rules = _rules_of(fields=fields, sid_fields=[], key=k.key_id)\n    req = DepseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().depseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.pseudonymize","title":"<code>pseudonymize(dataset, fields=None, sid_fields=None, sid_snapshot_date=None, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Pseudonymize specified fields of a dataset.</p> The dataset may be supplied as <ul> <li>A local file on disk (string or Path)</li> <li>A file handle (io.BufferedReader)</li> <li>A Pandas dataframe</li> </ul> <p>Supported file formats: json, csv</p> <p>The <code>fields</code> and <code>sid_fields</code> lists specify what to pseudonymize. At least one of these fields must be specified. The list contents can either be plain field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply more advanced techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>For <code>fields</code>, the <code>daead</code> pseudonymization function is used. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>For <code>sid_fields</code>, the <code>map-sid</code> pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be compatible with those produced by the PAPIS project.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param dataset: path to file, file handle or dataframe :param fields: list of fields that should be pseudonymized :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def pseudonymize(\n    dataset: DatasetDecl,\n    fields: t.Optional[t.List[FieldDecl]] = None,\n    sid_fields: t.Optional[t.List[str]] = None,\n    sid_snapshot_date: t.Optional[str | date] = None,\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    r\"\"\"Pseudonymize specified fields of a dataset.\n\n    The dataset may be supplied as:\n        - A local file on disk (string or Path)\n        - A file handle (io.BufferedReader)\n        - A Pandas dataframe\n\n    Supported file formats: json, csv\n\n    The ``fields`` and ``sid_fields`` lists specify what to pseudonymize. At least one of these fields must be specified.\n    The list contents can either be plain field names (e.g. ``[\"some_field1\", \"another_field2\"]``, or you can apply\n    more advanced techniques, such as using wildcard characters (e.g. *Name) or slashes to target hierarchical fields\n    (e.g. **/path/to/hierarchicalStuff).\n\n    For ``fields``, the ``daead`` pseudonymization function is used. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    For ``sid_fields``, the ``map-sid`` pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and\n    subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be\n    compatible with those produced by the PAPIS project.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param dataset: path to file, file handle or dataframe\n    :param fields: list of fields that should be pseudonymized\n    :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized\n    :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: pseudonymized data\n    \"\"\"\n    if not fields and not sid_fields:\n        raise ValueError(\"At least one of fields and sid_fields must be specified.\")\n\n    # Avoid later type errors by making sure we have lists\n    if fields is None:\n        fields = []\n    if sid_fields is None:\n        sid_fields = []\n\n    file_handle: t.Optional[BinaryFileDecl] = None\n    name: t.Optional[str] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            content_type = Mimetypes(magic.from_file(dataset, mime=True))\n        case pd.DataFrame():\n            # Dataframe\n            content_type = Mimetypes.JSON\n            file_handle = _dataframe_to_json(dataset, fields, sid_fields)\n        case io.BufferedReader():\n            # File handle\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            name = dataset.path.split(\"/\")[-1] if hasattr(dataset, \"path\") else None\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {DatasetDecl}\")\n    k = KeyWrapper(key)\n    sid_func_kwargs = MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)) if sid_fields else None\n    rules = _rules_of(\n        fields=fields,\n        sid_fields=sid_fields or [],\n        key=k.key_id,\n        sid_func_kwargs=sid_func_kwargs,\n    )\n    pseudonymize_request = PseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    if file_handle is not None:\n        return _client().pseudonymize_file(pseudonymize_request, file_handle, stream=stream, name=name, timeout=timeout)\n    else:\n        return _client()._process_file(\n            \"pseudonymize\",\n            pseudonymize_request,\n            str(dataset),\n            stream=stream,\n            timeout=timeout,\n        )\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/#dapla_pseudo.v1.repseudonymize","title":"<code>repseudonymize(file_path, fields, source_key=PredefinedKeys.SSB_COMMON_KEY_1, target_key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Repseudonymize specified fields of a local, previously pseudonymized file.</p> <p>You will need to provide a crypto key for both the source data and a key that should be used for re-pseudonymization.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json :param fields: list of fields that should be pseudonymized :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def repseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    source_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    target_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize specified fields of a local, previously pseudonymized file.\n\n    You will need to provide a crypto key for both the source data and a key that should be used for\n    re-pseudonymization.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json\n    :param fields: list of fields that should be pseudonymized\n    :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization\n    :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: repseudonymized data\n    \"\"\"\n    content_type = _content_type_of(file_path)\n    source_key_wrapper = KeyWrapper(source_key)\n    target_key_wrapper = KeyWrapper(target_key)\n    source_rules = _rules_of(fields=fields, sid_fields=[], key=source_key_wrapper.key_id)\n    target_rules = _rules_of(fields=fields, sid_fields=[], key=target_key_wrapper.key_id)\n    req = RepseudonymizeFileRequest(\n        source_pseudo_config=PseudoConfig(rules=source_rules, keysets=source_key_wrapper.keyset_list()),\n        target_pseudo_config=PseudoConfig(rules=target_rules, keysets=target_key_wrapper.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().repseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/","title":"builder_models","text":"<p>Common API models for builder packages.</p>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.PseudoFileResponse","title":"<code>PseudoFileResponse</code>  <code>dataclass</code>","text":"<p>PseudoFileResponse holds the data and metadata from a Pseudo Service file response.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>@dataclass\nclass PseudoFileResponse:\n    \"\"\"PseudoFileResponse holds the data and metadata from a Pseudo Service file response.\"\"\"\n\n    response: Response\n    content_type: Mimetypes\n    streamed: bool = True\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result","title":"<code>Result</code>","text":"<p>Result represents the result of a pseudonymization operation.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>class Result:\n    \"\"\"Result represents the result of a pseudonymization operation.\"\"\"\n\n    def __init__(\n        self,\n        pseudo_response: pl.DataFrame | PseudoFileResponse,\n        metadata: Optional[t.Dict[str, str]] = None,\n    ) -&gt; None:\n        \"\"\"Initialise a PseudonymizationResult.\"\"\"\n        self._pseudo_response = pseudo_response\n        self._metadata = metadata if metadata else {}\n\n    def to_polars(self, **kwargs: t.Any) -&gt; pl.DataFrame:\n        \"\"\"Output pseudonymized data as a Polars DataFrame.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed the Polars reader function *if* the input data is from a file.\n                The specific reader function depends on the format, e.g. `read_csv` for CSV files.\n\n        Raises:\n            ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n\n        Returns:\n            pl.DataFrame: A Polars DataFrame containing the pseudonymized data.\n        \"\"\"\n        match self._pseudo_response:\n            case pl.DataFrame():\n                return self._pseudo_response\n            case PseudoFileResponse(response, content_type, _):\n                format = SupportedOutputFileFormat(content_type.name.lower())\n                df = read_to_polars_df(format, BytesIO(response.content), **kwargs)\n                return df\n            case _:\n                raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n\n    def to_pandas(self, **kwargs: t.Any) -&gt; pd.DataFrame:\n        \"\"\"Output pseudonymized data as a Pandas DataFrame.\n\n        Args:\n            **kwargs: Additional keyword arguments to be passed the Pandas reader function *if* the input data is from a file.\n                The specific reader function depends on the format of the input file, e.g. `read_csv()` for CSV files.\n\n        Raises:\n            ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n\n        Returns:\n            pd.DataFrame: A Pandas DataFrame containing the pseudonymized data.\n        \"\"\"\n        match self._pseudo_response:\n            case pl.DataFrame():\n                return self._pseudo_response.to_pandas()\n            case PseudoFileResponse(response, content_type, _):\n                format = SupportedOutputFileFormat(content_type.name.lower())\n                df = read_to_pandas_df(format, BytesIO(response.content), **kwargs)\n                return df\n            case _:\n                raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n\n    def to_file(self, file_path: str | Path, **kwargs: t.Any) -&gt; None:\n        \"\"\"Write pseudonymized data to a file.\n\n        Args:\n            file_path (str | Path): The path to the file to be written.\n            **kwargs: Additional keyword arguments to be passed the Polars writer function *if* the input data is a DataFrame.\n                The specific writer function depends on the format of the output file, e.g. `write_csv()` for CSV files.\n\n        Raises:\n            ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n            ValueError: If the output file format does not match the input file format.\n\n        \"\"\"\n        file_format = get_file_format_from_file_name(file_path)\n\n        if str(file_path).startswith(\"gs://\"):\n            file_handle = FileClient().gcs_open(file_path, mode=\"wb\")\n        else:\n            file_handle = open(file_path, mode=\"wb\")\n\n        match self._pseudo_response:\n            case PseudoFileResponse(response, content_type, streamed):\n                if FORMAT_TO_MIMETYPE_FUNCTION[file_format] != content_type:\n                    raise ValueError(\n                        f'Provided output file format \"{file_format}\" does not'\n                        f'match the content type of the provided input file \"{content_type.name}\".'\n                    )\n                if streamed:\n                    for chunk in response.iter_content(chunk_size=128):\n                        file_handle.write(chunk)\n                else:\n                    file_handle.write(self._pseudo_response.response.content)\n            case pl.DataFrame():\n                write_from_df(self._pseudo_response, file_format, file_path, **kwargs)\n            case _:\n                raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n\n        file_handle.close()\n\n    @property\n    def metadata(self) -&gt; dict[str, str]:\n        \"\"\"Returns the pseudonymization metadata as a dictionary.\n\n        Returns:\n            Optional[dict[str, str]]: A dictionary containing the pseudonymization metadata,\n            where the keys are field names and the values are corresponding pseudo field metadata.\n            If no metadata is set, returns an empty dictionary.\n        \"\"\"\n        return self._metadata\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result.metadata","title":"<code>metadata: dict[str, str]</code>  <code>property</code>","text":"<p>Returns the pseudonymization metadata as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Optional[dict[str, str]]: A dictionary containing the pseudonymization metadata,</p> <code>dict[str, str]</code> <p>where the keys are field names and the values are corresponding pseudo field metadata.</p> <code>dict[str, str]</code> <p>If no metadata is set, returns an empty dictionary.</p>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result.__init__","title":"<code>__init__(pseudo_response, metadata=None)</code>","text":"<p>Initialise a PseudonymizationResult.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>def __init__(\n    self,\n    pseudo_response: pl.DataFrame | PseudoFileResponse,\n    metadata: Optional[t.Dict[str, str]] = None,\n) -&gt; None:\n    \"\"\"Initialise a PseudonymizationResult.\"\"\"\n    self._pseudo_response = pseudo_response\n    self._metadata = metadata if metadata else {}\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result.to_file","title":"<code>to_file(file_path, **kwargs)</code>","text":"<p>Write pseudonymized data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>The path to the file to be written.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed the Polars writer function if the input data is a DataFrame. The specific writer function depends on the format of the output file, e.g. <code>write_csv()</code> for CSV files.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not of type Polars DataFrame or PseudoFileResponse.</p> <code>ValueError</code> <p>If the output file format does not match the input file format.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>def to_file(self, file_path: str | Path, **kwargs: t.Any) -&gt; None:\n    \"\"\"Write pseudonymized data to a file.\n\n    Args:\n        file_path (str | Path): The path to the file to be written.\n        **kwargs: Additional keyword arguments to be passed the Polars writer function *if* the input data is a DataFrame.\n            The specific writer function depends on the format of the output file, e.g. `write_csv()` for CSV files.\n\n    Raises:\n        ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n        ValueError: If the output file format does not match the input file format.\n\n    \"\"\"\n    file_format = get_file_format_from_file_name(file_path)\n\n    if str(file_path).startswith(\"gs://\"):\n        file_handle = FileClient().gcs_open(file_path, mode=\"wb\")\n    else:\n        file_handle = open(file_path, mode=\"wb\")\n\n    match self._pseudo_response:\n        case PseudoFileResponse(response, content_type, streamed):\n            if FORMAT_TO_MIMETYPE_FUNCTION[file_format] != content_type:\n                raise ValueError(\n                    f'Provided output file format \"{file_format}\" does not'\n                    f'match the content type of the provided input file \"{content_type.name}\".'\n                )\n            if streamed:\n                for chunk in response.iter_content(chunk_size=128):\n                    file_handle.write(chunk)\n            else:\n                file_handle.write(self._pseudo_response.response.content)\n        case pl.DataFrame():\n            write_from_df(self._pseudo_response, file_format, file_path, **kwargs)\n        case _:\n            raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n\n    file_handle.close()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result.to_pandas","title":"<code>to_pandas(**kwargs)</code>","text":"<p>Output pseudonymized data as a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed the Pandas reader function if the input data is from a file. The specific reader function depends on the format of the input file, e.g. <code>read_csv()</code> for CSV files.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not of type Polars DataFrame or PseudoFileResponse.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A Pandas DataFrame containing the pseudonymized data.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>def to_pandas(self, **kwargs: t.Any) -&gt; pd.DataFrame:\n    \"\"\"Output pseudonymized data as a Pandas DataFrame.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed the Pandas reader function *if* the input data is from a file.\n            The specific reader function depends on the format of the input file, e.g. `read_csv()` for CSV files.\n\n    Raises:\n        ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the pseudonymized data.\n    \"\"\"\n    match self._pseudo_response:\n        case pl.DataFrame():\n            return self._pseudo_response.to_pandas()\n        case PseudoFileResponse(response, content_type, _):\n            format = SupportedOutputFileFormat(content_type.name.lower())\n            df = read_to_pandas_df(format, BytesIO(response.content), **kwargs)\n            return df\n        case _:\n            raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_models/#dapla_pseudo.v1.builder_models.Result.to_polars","title":"<code>to_polars(**kwargs)</code>","text":"<p>Output pseudonymized data as a Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed the Polars reader function if the input data is from a file. The specific reader function depends on the format, e.g. <code>read_csv</code> for CSV files.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not of type Polars DataFrame or PseudoFileResponse.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: A Polars DataFrame containing the pseudonymized data.</p> Source code in <code>dapla_pseudo/v1/builder_models.py</code> <pre><code>def to_polars(self, **kwargs: t.Any) -&gt; pl.DataFrame:\n    \"\"\"Output pseudonymized data as a Polars DataFrame.\n\n    Args:\n        **kwargs: Additional keyword arguments to be passed the Polars reader function *if* the input data is from a file.\n            The specific reader function depends on the format, e.g. `read_csv` for CSV files.\n\n    Raises:\n        ValueError: If the result is not of type Polars DataFrame or PseudoFileResponse.\n\n    Returns:\n        pl.DataFrame: A Polars DataFrame containing the pseudonymized data.\n    \"\"\"\n    match self._pseudo_response:\n        case pl.DataFrame():\n            return self._pseudo_response\n        case PseudoFileResponse(response, content_type, _):\n            format = SupportedOutputFileFormat(content_type.name.lower())\n            df = read_to_polars_df(format, BytesIO(response.content), **kwargs)\n            return df\n        case _:\n            raise ValueError(f\"Invalid response type: {type(self._pseudo_response)}\")\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/","title":"builder_pseudo","text":"<p>Builder for submitting a pseudonymization request.</p>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.File","title":"<code>File</code>  <code>dataclass</code>","text":"<p>File represents a file to be pseudonymized.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@dataclass\nclass File:\n    \"\"\"File represents a file to be pseudonymized.\"\"\"\n\n    file_handle: BinaryFileDecl\n    content_type: Mimetypes\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData","title":"<code>PseudoData</code>","text":"<p>Starting point for pseudonymization of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>class PseudoData:\n    \"\"\"Starting point for pseudonymization of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    dataset: File | pl.DataFrame\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n        dataset: pl.DataFrame = pl.from_pandas(dataframe)\n        PseudoData.dataset = dataset\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n        PseudoData.dataset = dataframe\n        return PseudoData._Pseudonymizer()\n\n    @staticmethod\n    def from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n        \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n        Args:\n            dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified path.\n            FileInvalidError: If the file is empty.\n            ValueError: If the dataset is not of a supported type.\n            DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n        Returns:\n            _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import PseudoData\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            field_selector = PseudoData.from_file(bucket_path)\n\n            # Read from local filesystem\n            from dapla_pseudo import PseudoData\n\n            local_path = \"some_file.csv\"\n            field_selector = PseudoData.from_file(local_path))\n        \"\"\"\n        file_handle: t.Optional[BinaryFileDecl] = None\n        match dataset:\n            case str() | Path():\n                # File path\n                if str(dataset).startswith(\"gs://\"):\n                    try:\n                        file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                    except OSError as err:\n                        raise FileNotFoundError(\n                            f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                        ) from err\n                    except DefaultCredentialsError as err:\n                        raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n                else:\n                    file_handle = open(dataset, \"rb\")\n\n                file_handle.seek(0)\n\n            case io.BufferedReader():\n                # File handle\n                dataset.seek(0)\n                file_handle = dataset\n            case fsspec.spec.AbstractBufferedFile():\n                # This is a file handle to a remote storage system such as GCS.\n                # It provides random access for the underlying file-like data (without downloading the whole thing).\n                dataset.seek(0)\n                file_handle = io.BufferedReader(dataset)\n            case _:\n                raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n        if isinstance(file_handle, GCSFile):\n            file_size = file_handle.size\n        else:\n            file_size = os.fstat(file_handle.fileno()).st_size\n\n        if file_size == 0:\n            raise FileInvalidError(\"File is empty.\")\n\n        content_type = _get_content_type_from_file(file_handle)\n        PseudoData.dataset = File(file_handle, content_type)\n        return PseudoData._Pseudonymizer()\n\n    class _Pseudonymizer:\n        \"\"\"Select one or multiple fields to be pseudonymized.\"\"\"\n\n        def __init__(self, rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            \"\"\"Initialize the class.\"\"\"\n            self._rules: list[PseudoRule] = [] if rules is None else rules\n            self._pseudo_keyset: Optional[PseudoKeyset] = None\n            self._metadata: t.Dict[str, str] = {}\n            self._timeout: int = TIMEOUT_DEFAULT\n\n        def on_fields(self, *fields: str) -&gt; \"PseudoData._PseudoFuncSelector\":\n            \"\"\"Specify one or multiple fields to be pseudonymized.\"\"\"\n            return PseudoData._PseudoFuncSelector(list(fields), self._rules)\n\n        def pseudonymize(\n            self,\n            with_custom_keyset: Optional[PseudoKeyset] = None,\n            timeout: int = TIMEOUT_DEFAULT,\n        ) -&gt; Result:\n            \"\"\"Pseudonymize the dataset.\n\n            Args:\n                with_custom_keyset (PseudoKeyset, optional): The pseudonymization keyset to use. Defaults to None.\n                timeout (int): The timeout in seconds for the API call. Defaults to TIMEOUT_DEFAULT.\n\n            Raises:\n                ValueError: If no dataset has been provided, no fields have been provided, or the dataset is of an unsupported type.\n\n            Returns:\n                Result: The pseudonymized dataset and the associated metadata.\n            \"\"\"\n            if PseudoData.dataset is None:\n                raise ValueError(\"No dataset has been provided.\")\n\n            if self._rules == []:\n                raise ValueError(\"No fields have been provided. Use the 'on_fields' method.\")\n\n            if with_custom_keyset is not None:\n                self._pseudo_keyset = with_custom_keyset\n\n            self._timeout = timeout\n            match PseudoData.dataset:  # Differentiate between file and DataFrame\n                case File():\n                    return self._pseudonymize_file()\n                case pl.DataFrame():\n                    return self._pseudonymize_field()\n                case _:\n                    raise ValueError(\n                        f\"Unsupported data type: {type(PseudoData.dataset)}. Should only be DataFrame or file-like type.\"\n                    )\n\n        def _pseudonymize_file(self) -&gt; Result:\n            \"\"\"Pseudonymize the entire file.\"\"\"\n            # Need to type-cast explicitly. We know that PseudoData.dataset is a \"File\" if we reach this method.\n            file = t.cast(File, PseudoData.dataset)\n\n            pseudonymize_request = PseudonymizeFileRequest(\n                pseudo_config=PseudoConfig(\n                    rules=self._rules,\n                    keysets=KeyWrapper(self._pseudo_keyset).keyset_list(),\n                ),\n                target_content_type=file.content_type,\n                target_uri=None,\n                compression=None,\n            )\n\n            response: Response = _client().pseudonymize_file(\n                pseudonymize_request,\n                file.file_handle,\n                stream=True,\n                name=None,\n                timeout=self._timeout,\n            )\n            return Result(\n                PseudoFileResponse(response, file.content_type, streamed=True),\n                self._metadata,\n            )\n\n        def _pseudonymize_field(self) -&gt; Result:\n            \"\"\"Pseudonymizes the specified fields in the DataFrame using the provided pseudonymization function.\n\n            The pseudonymization is performed in parallel. After the parallel processing is finished,\n            the pseudonymized fields replace the original fields in the DataFrame stored in `self._dataframe`.\n\n            Returns:\n                Result: Containing the pseudonymized 'self._dataframe' and the associated metadata.\n            \"\"\"\n\n            def pseudonymize_field_runner(\n                field_name: str, series: pl.Series, pseudo_func: PseudoFunction\n            ) -&gt; tuple[str, pl.Series]:\n                \"\"\"Function that performs the pseudonymization on a pandas Series.\n\n                Args:\n                    field_name (str):  The name of the field.\n                    series (pl.Series): The pandas Series containing the values to be pseudonymized.\n                    pseudo_func (PseudoFunction): The pseudonymization function to apply to the values.\n\n                Returns:\n                    tuple[str,pl.Series]: A tuple containing the field_name and the corresponding series.\n                \"\"\"\n                return (\n                    field_name,\n                    _do_pseudonymize_field(\n                        path=\"pseudonymize/field\",\n                        field_name=field_name,\n                        values=series.to_list(),\n                        pseudo_func=pseudo_func,\n                        metadata_map=self._metadata,\n                        timeout=self._timeout,\n                        keyset=self._pseudo_keyset,\n                    ),\n                )\n\n            dataframe = t.cast(pd.DataFrame, PseudoData.dataset)\n            # Execute the pseudonymization API calls in parallel\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                pseudonymized_field: t.Dict[str, pl.Series] = {}\n                futures = [\n                    executor.submit(\n                        pseudonymize_field_runner,\n                        rule.pattern,\n                        dataframe[rule.pattern],\n                        rule.func,\n                    )\n                    for rule in self._rules\n                ]\n                # Wait for the futures to finish, then add each field to pseudonymized_field map\n                for future in concurrent.futures.as_completed(futures):\n                    result = future.result()\n                    # Each future result contains the field_name (0) and the pseudonymize_values (1)\n                    pseudonymized_field[result[0]] = result[1]\n\n                pseudonymized_df = pl.DataFrame(pseudonymized_field)\n                dataframe = dataframe.update(pseudonymized_df)\n\n            return Result(pseudo_response=dataframe, metadata=self._metadata)\n\n    class _PseudoFuncSelector:\n        def __init__(self, fields: list[str], rules: Optional[list[PseudoRule]] = None) -&gt; None:\n            self._fields = fields\n            self._existing_rules = [] if rules is None else rules\n\n        def with_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; \"PseudoData._Pseudonymizer\":\n            \"\"\"Map selected fields to stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Self: The object configured to be mapped to stable ID\n            \"\"\"\n            function = PseudoFunction(\n                function_type=PseudoFunctionTypes.MAP_SID,\n                kwargs=MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)),\n            )\n            return self._rule_constructor(function)\n\n        def with_default_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.DAEAD, kwargs=DaeadKeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_papis_compatible_encryption(self) -&gt; \"PseudoData._Pseudonymizer\":\n            function = PseudoFunction(function_type=PseudoFunctionTypes.FF31, kwargs=FF31KeywordArgs())\n            return self._rule_constructor(function)\n\n        def with_custom_function(self, function: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            return self._rule_constructor(function)\n\n        def _rule_constructor(self, func: PseudoFunction) -&gt; \"PseudoData._Pseudonymizer\":\n            # If we use the pseudonymize_file endpoint, we need a glob catch-all prefix.\n            rule_prefix = \"**/\" if isinstance(PseudoData.dataset, File) else \"\"\n            rules = [PseudoRule(name=None, func=func, pattern=f\"{rule_prefix}{field}\") for field in self._fields]\n            return PseudoData._Pseudonymizer(self._existing_rules + rules)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData.from_file","title":"<code>from_file(dataset)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>FileLikeDatasetDecl</code> <p>Either a path to the file to be read, or a file handle.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified path.</p> <code>FileInvalidError</code> <p>If the file is empty.</p> <code>ValueError</code> <p>If the dataset is not of a supported type.</p> <code>DefaultCredentialsError</code> <p>If no Google Authentication is found in the environment.</p> <p>Returns:</p> Name Type Description <code>_Pseudonymizer</code> <code>_Pseudonymizer</code> <p>An instance of the _Pseudonymizer class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import PseudoData bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" field_selector = PseudoData.from_file(bucket_path)</p>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import PseudoData</p> <p>local_path = \"some_file.csv\" field_selector = PseudoData.from_file(local_path))</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_file(dataset: FileLikeDatasetDecl) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas dataframe read from file.\n\n    Args:\n        dataset (FileLikeDatasetDecl): Either a path to the file to be read, or a file handle.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified path.\n        FileInvalidError: If the file is empty.\n        ValueError: If the dataset is not of a supported type.\n        DefaultCredentialsError: If no Google Authentication is found in the environment.\n\n    Returns:\n        _Pseudonymizer: An instance of the _Pseudonymizer class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import PseudoData\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        field_selector = PseudoData.from_file(bucket_path)\n\n        # Read from local filesystem\n        from dapla_pseudo import PseudoData\n\n        local_path = \"some_file.csv\"\n        field_selector = PseudoData.from_file(local_path))\n    \"\"\"\n    file_handle: t.Optional[BinaryFileDecl] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            if str(dataset).startswith(\"gs://\"):\n                try:\n                    file_handle = FileClient().gcs_open(dataset, mode=\"rb\")\n                except OSError as err:\n                    raise FileNotFoundError(\n                        f\"No GCS file found or authentication not sufficient for: {dataset}\"\n                    ) from err\n                except DefaultCredentialsError as err:\n                    raise DefaultCredentialsError(\"No Google Authentication found in environment\") from err\n            else:\n                file_handle = open(dataset, \"rb\")\n\n            file_handle.seek(0)\n\n        case io.BufferedReader():\n            # File handle\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {FileLikeDatasetDecl}\")\n\n    if isinstance(file_handle, GCSFile):\n        file_size = file_handle.size\n    else:\n        file_size = os.fstat(file_handle.fileno()).st_size\n\n    if file_size == 0:\n        raise FileInvalidError(\"File is empty.\")\n\n    content_type = _get_content_type_from_file(file_handle)\n    PseudoData.dataset = File(file_handle, content_type)\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a pandas DataFrame.\"\"\"\n    dataset: pl.DataFrame = pl.from_pandas(dataframe)\n    PseudoData.dataset = dataset\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_pseudo/#dapla_pseudo.v1.builder_pseudo.PseudoData.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a pseudonymization request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_pseudo.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"PseudoData._Pseudonymizer\":\n    \"\"\"Initialize a pseudonymization request from a polars DataFrame.\"\"\"\n    PseudoData.dataset = dataframe\n    return PseudoData._Pseudonymizer()\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_validation/","title":"builder_validation","text":"<p>Builder for submitting a validation request.</p>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator","title":"<code>Validator</code>","text":"<p>Starting point for validation of datasets.</p> <p>This class should not be instantiated, only the static methods should be used.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>class Validator:\n    \"\"\"Starting point for validation of datasets.\n\n    This class should not be instantiated, only the static methods should be used.\n    \"\"\"\n\n    @staticmethod\n    def from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n        return Validator._FieldSelector(dataframe)\n\n    @staticmethod\n    def from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n        \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n        Args:\n            file_path_str (str): The path to the file to be read.\n            kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n        Raises:\n            FileNotFoundError: If no file is found at the specified local path.\n\n        Returns:\n            _FieldSelector: An instance of the _FieldSelector class.\n\n        Examples:\n            # Read from bucket\n            from dapla import AuthClient\n            from dapla_pseudo import Validator\n            bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n            storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n            field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n            # Read from local filesystem\n            from dapla_pseudo import Validator\n\n            local_path = \"some_file.csv\"\n            field_selector = Validator.from_file(local_path)\n        \"\"\"\n        file_path = Path(file_path_str)\n\n        if not file_path.is_file() and \"storage_options\" not in kwargs:\n            raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n        file_format = get_file_format_from_file_name(file_path)\n\n        return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n\n    class _FieldSelector:\n        \"\"\"Select a field to be validated.\"\"\"\n\n        def __init__(self, dataframe: pd.DataFrame | pl.DataFrame):\n            \"\"\"Initialize the class.\"\"\"\n            self._dataframe: pl.DataFrame\n            if isinstance(dataframe, pd.DataFrame):\n                self._dataframe = pl.from_pandas(dataframe)\n            else:\n                self._dataframe = dataframe\n\n        def on_field(self, field: str) -&gt; \"Validator._Validator\":\n            \"\"\"Specify a single field to be validated.\"\"\"\n            return Validator._Validator(self._dataframe, field)\n\n    class _Validator:\n        \"\"\"Assemble the validation request.\"\"\"\n\n        def __init__(\n            self,\n            dataframe: pl.DataFrame,\n            field: str,\n        ) -&gt; None:\n            self._dataframe: pl.DataFrame = dataframe\n            self._field: str = field\n\n        def validate_map_to_stable_id(self, sid_snapshot_date: Optional[str | date] = None) -&gt; Result:\n            \"\"\"Checks if all the selected fields can be mapped to a stable ID.\n\n            Args:\n                sid_snapshot_date (Optional[str | date], optional): Date representing SID-catalogue version to use.\n                    Latest if unspecified. Format: YYYY-MM-DD\n\n            Returns:\n                Result: Containing a result dataframe with associated metadata.\n            \"\"\"\n            response: requests.Response = _client()._post_to_sid_endpoint(\n                \"sid/lookup/batch\",\n                self._dataframe[self._field].to_list(),\n                sid_snapshot_date,\n                stream=True,\n            )\n            # The response content is received as a buffered byte stream from the server.\n            # We decode the content using UTF-8, which gives us a List[Dict[str]] structure.\n            result_json = json.loads(response.content.decode(\"utf-8\"))[0]\n            result: Sequence[str] = []\n            metadata: dict[str, str] = {}\n            if \"missing\" in result_json:\n                result = result_json[\"missing\"]\n            if \"datasetExtractionSnapshotTime\" in result_json:\n                metadata = {\"datasetExtractionSnapshotTime\": result_json[\"datasetExtractionSnapshotTime\"]}\n\n            result_df = pl.DataFrame(pl.Series(self._field, result))\n            return Result(pseudo_response=result_df, metadata=metadata)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator.from_file","title":"<code>from_file(file_path_str, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas dataframe read from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path_str</code> <code>str</code> <p>The path to the file to be read.</p> required <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the file reader.</p> <code>{}</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no file is found at the specified local path.</p> <p>Returns:</p> Name Type Description <code>_FieldSelector</code> <code>_FieldSelector</code> <p>An instance of the _FieldSelector class.</p> <p>Examples:</p>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator.from_file--read-from-bucket","title":"Read from bucket","text":"<p>from dapla import AuthClient from dapla_pseudo import Validator bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\" storage_options = {\"token\": AuthClient.fetch_google_credentials()} field_selector = Validator.from_file(bucket_path, storage_options=storage_options)</p>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator.from_file--read-from-local-filesystem","title":"Read from local filesystem","text":"<p>from dapla_pseudo import Validator</p> <p>local_path = \"some_file.csv\" field_selector = Validator.from_file(local_path)</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_file(file_path_str: str, **kwargs: Any) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas dataframe read from file.\n\n    Args:\n        file_path_str (str): The path to the file to be read.\n        kwargs (dict): Additional keyword arguments to be passed to the file reader.\n\n    Raises:\n        FileNotFoundError: If no file is found at the specified local path.\n\n    Returns:\n        _FieldSelector: An instance of the _FieldSelector class.\n\n    Examples:\n        # Read from bucket\n        from dapla import AuthClient\n        from dapla_pseudo import Validator\n        bucket_path = \"gs://ssb-staging-dapla-felles-data-delt/felles/smoke-tests/fruits/data.parquet\"\n        storage_options = {\"token\": AuthClient.fetch_google_credentials()}\n        field_selector = Validator.from_file(bucket_path, storage_options=storage_options)\n\n        # Read from local filesystem\n        from dapla_pseudo import Validator\n\n        local_path = \"some_file.csv\"\n        field_selector = Validator.from_file(local_path)\n    \"\"\"\n    file_path = Path(file_path_str)\n\n    if not file_path.is_file() and \"storage_options\" not in kwargs:\n        raise FileNotFoundError(f\"No local file found in path: {file_path}\")\n\n    file_format = get_file_format_from_file_name(file_path)\n\n    return Validator._FieldSelector(read_to_polars_df(file_format, file_path, **kwargs))\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator.from_pandas","title":"<code>from_pandas(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a pandas DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_pandas(dataframe: pd.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a pandas DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/builder_validation/#dapla_pseudo.v1.builder_validation.Validator.from_polars","title":"<code>from_polars(dataframe)</code>  <code>staticmethod</code>","text":"<p>Initialize a validation request from a polars DataFrame.</p> Source code in <code>dapla_pseudo/v1/builder_validation.py</code> <pre><code>@staticmethod\ndef from_polars(dataframe: pl.DataFrame) -&gt; \"Validator._FieldSelector\":\n    \"\"\"Initialize a validation request from a polars DataFrame.\"\"\"\n    return Validator._FieldSelector(dataframe)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/","title":"client","text":"<p>Module that implements a client abstraction that makes it easy to communicate with the Dapla Pseudo Service REST API.</p>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient","title":"<code>PseudoClient</code>","text":"<p>Client for interacting with the Dapla Pseudo Service REST API.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>class PseudoClient:\n    \"\"\"Client for interacting with the Dapla Pseudo Service REST API.\"\"\"\n\n    PSEUDONYMIZE_FILE_ENDPOINT = \"pseudonymize/file\"\n\n    def __init__(\n        self,\n        pseudo_service_url: t.Optional[str] = None,\n        auth_token: t.Optional[str] = None,\n    ):\n        \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n        self.pseudo_service_url = (\n            \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n        )\n        self.static_auth_token = auth_token\n\n    def __auth_token(self) -&gt; str:\n        return str(AuthClient.fetch_personal_token()) if self.static_auth_token is None else str(self.static_auth_token)\n\n    def pseudonymize_file(\n        self,\n        pseudonymize_request: PseudonymizeFileRequest,\n        data: BinaryFileDecl,\n        timeout: int,\n        stream: bool = False,\n        name: t.Optional[str] = None,\n    ) -&gt; requests.Response:\n        \"\"\"Pseudonymize data from a file-like object.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a glob\n        (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n        :param pseudonymize_request: the request to send to Dapla Pseudo Service\n        :param data: file handle that should be pseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :param name: optional name for logging purposes\n        :return: pseudonymized data\n        \"\"\"\n        return self._post_to_file_endpoint(\n            self.PSEUDONYMIZE_FILE_ENDPOINT,\n            pseudonymize_request,\n            data,\n            self._extract_name(data, pseudonymize_request.target_content_type, name),\n            pseudonymize_request.target_content_type,\n            timeout,\n            stream,\n        )\n\n    def _extract_name(self, data: t.BinaryIO, content_type: Mimetypes, name: t.Optional[str]) -&gt; str:\n        if name is None:\n            try:\n                name = data.name\n            except AttributeError:\n                # Fallback to default name\n                name = \"unknown\"\n\n        if not name.endswith(\".json\") and content_type is Mimetypes.JSON:\n            name = f\"{name}.json\"  # Pseudo service expects a file extension\n\n        if \"/\" in name:\n            name = name.split(\"/\")[-1]  # Pseudo service expects a file name, not a path\n\n        return name\n\n    def depseudonymize_file(\n        self,\n        depseudonymize_request: DepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Notice that only certain whitelisted users can depseudonymize data.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: depseudonymized data\n        \"\"\"\n        return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n\n    def repseudonymize_file(\n        self,\n        repseudonymize_request: RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n        Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n        Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n        Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n        formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n        Reduce transmission times by applying compression both to the source and target files.\n        Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n        Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n        processed. Each rule defines a `pattern` (as a\n        glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n        fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n        defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n        are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n        of rules are linked to separate keysets.\n\n        Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n        param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :param file_path: path to a local file that should be depseudonymized\n        :param timeout: connection and read timeout, see\n            https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n        :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n        :return: repseudonymized data\n        \"\"\"\n        return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n\n    def _process_file(\n        self,\n        operation: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        file_path: str,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        file_name = os.path.basename(file_path).split(\"/\")[-1]\n        content_type = Mimetypes(mimetypes.MimeTypes().guess_type(file_path)[0])\n\n        with open(file_path, \"rb\") as f:\n            return self._post_to_file_endpoint(\n                f\"{operation}/file\",\n                request,\n                f,\n                file_name,\n                content_type,\n                timeout,\n                stream,\n            )\n\n    def _post_to_file_endpoint(\n        self,\n        path: str,\n        request: PseudonymizeFileRequest | DepseudonymizeFileRequest | RepseudonymizeFileRequest,\n        data: t.BinaryIO,\n        name: str,\n        content_type: Mimetypes,\n        timeout: int,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        data_spec: FileSpecDecl = (name, data, content_type)\n        request_spec: FileSpecDecl = (None, request.to_json(), str(Mimetypes.JSON))\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Accept-Encoding\": \"gzip\",\n            },\n            files={\n                \"data\": data_spec,\n                \"request\": request_spec,\n            },\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_field_endpoint(\n        self,\n        path: str,\n        field_name: str,\n        values: list[str],\n        pseudo_func: t.Optional[PseudoFunction],\n        timeout: int,\n        keyset: t.Optional[PseudoKeyset] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\n            \"name\": field_name,\n            \"values\": values,\n            \"pseudoFunc\": str(pseudo_func),\n        }\n        if keyset:\n            request[\"keyset\"] = {\n                \"kekUri\": keyset.kek_uri,\n                \"encryptedKeyset\": keyset.encrypted_keyset,\n                \"keysetInfo\": keyset.keyset_info,\n            }\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            headers={\n                \"Authorization\": f\"Bearer {self.__auth_token()}\",\n                \"Content-Type\": str(Mimetypes.JSON),\n            },\n            json=request,\n            stream=stream,\n            timeout=timeout,\n        )\n        response.raise_for_status()\n        return response\n\n    def _post_to_sid_endpoint(\n        self,\n        path: str,\n        values: list[str],\n        sid_snapshot_date: t.Optional[str | date] = None,\n        stream: bool = False,\n    ) -&gt; requests.Response:\n        request: t.Dict[str, t.Collection[str]] = {\"fnrList\": values}\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/{path}\",\n            params={\"snapshot\": str(sid_snapshot_date)} if sid_snapshot_date else None,\n            # Do not set content-type, as this will cause the json to serialize incorrectly\n            headers={\"Authorization\": f\"Bearer {self.__auth_token()}\"},\n            json=request,\n            stream=stream,\n            timeout=TIMEOUT_DEFAULT,  # seconds\n        )\n        response.raise_for_status()\n        return response\n\n    def export_dataset(self, request_json: str) -&gt; requests.Response:\n        \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n        The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n        It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n        This can be applied as a means to perform data minimization.\n\n        Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n        configuration and cannot be overridden.\n\n        See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n        :param request_json: the request JSON to send to Dapla Pseudo Service\n        :return: JSON response with a reference to the export \"job\"\n        \"\"\"\n        auth_token = self.__auth_token()\n        response = requests.post(\n            url=f\"{self.pseudo_service_url}/export\",\n            headers={\n                \"Authorization\": f\"Bearer {auth_token}\",\n                \"Content-Type\": \"application/json\",\n            },\n            data=request_json,\n            timeout=30,  # seconds\n        )\n        response.raise_for_status()\n        return response\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient.__init__","title":"<code>__init__(pseudo_service_url=None, auth_token=None)</code>","text":"<p>Use a default url for dapla-pseudo-service if not explicitly set.</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def __init__(\n    self,\n    pseudo_service_url: t.Optional[str] = None,\n    auth_token: t.Optional[str] = None,\n):\n    \"\"\"Use a default url for dapla-pseudo-service if not explicitly set.\"\"\"\n    self.pseudo_service_url = (\n        \"http://dapla-pseudo-service.dapla.svc.cluster.local\" if pseudo_service_url is None else pseudo_service_url\n    )\n    self.static_auth_token = auth_token\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient.depseudonymize_file","title":"<code>depseudonymize_file(depseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Notice that only certain whitelisted users can depseudonymize data.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def depseudonymize_file(\n    self,\n    depseudonymize_request: DepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Notice that only certain whitelisted users can depseudonymize data.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Depseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/depseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: depseudonymized data\n    \"\"\"\n    return self._process_file(\"depseudonymize\", depseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient.export_dataset","title":"<code>export_dataset(request_json)</code>","text":"<p>Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.</p> <p>The dataset will be archived in an encrypted zip file protected by a user provided password.</p> <p>It is possible to specify <code>columnSelectors</code>, that allows for partial export, e.g. only specific fields. This can be applied as a means to perform data minimization.</p> <p>Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application configuration and cannot be overridden.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :return: JSON response with a reference to the export \"job\"</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def export_dataset(self, request_json: str) -&gt; requests.Response:\n    \"\"\"Export a dataset in GCS to CSV or JSON, and optionally depseudonymize the data.\n\n    The dataset will be archived in an encrypted zip file protected by a user provided password.\n\n    It is possible to specify `columnSelectors`, that allows for partial export, e.g. only specific fields.\n    This can be applied as a means to perform data minimization.\n\n    Data is exported and stored to a specific, predefined GCS bucket. This is specified in the application\n    configuration and cannot be overridden.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/export\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :return: JSON response with a reference to the export \"job\"\n    \"\"\"\n    auth_token = self.__auth_token()\n    response = requests.post(\n        url=f\"{self.pseudo_service_url}/export\",\n        headers={\n            \"Authorization\": f\"Bearer {auth_token}\",\n            \"Content-Type\": \"application/json\",\n        },\n        data=request_json,\n        timeout=30,  # seconds\n    )\n    response.raise_for_status()\n    return response\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient.pseudonymize_file","title":"<code>pseudonymize_file(pseudonymize_request, data, timeout, stream=False, name=None)</code>","text":"<p>Pseudonymize data from a file-like object.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile</p> <p>:param pseudonymize_request: the request to send to Dapla Pseudo Service :param data: file handle that should be pseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :param name: optional name for logging purposes :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def pseudonymize_file(\n    self,\n    pseudonymize_request: PseudonymizeFileRequest,\n    data: BinaryFileDecl,\n    timeout: int,\n    stream: bool = False,\n    name: t.Optional[str] = None,\n) -&gt; requests.Response:\n    \"\"\"Pseudonymize data from a file-like object.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Pseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a glob\n    (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important).\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/pseudonymizeFile\n\n    :param pseudonymize_request: the request to send to Dapla Pseudo Service\n    :param data: file handle that should be pseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :param name: optional name for logging purposes\n    :return: pseudonymized data\n    \"\"\"\n    return self._post_to_file_endpoint(\n        self.PSEUDONYMIZE_FILE_ENDPOINT,\n        pseudonymize_request,\n        data,\n        self._extract_name(data, pseudonymize_request.target_content_type, name),\n        pseudonymize_request.target_content_type,\n        timeout,\n        stream,\n    )\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/client/#dapla_pseudo.v1.client.PseudoClient.repseudonymize_file","title":"<code>repseudonymize_file(repseudonymize_request, file_path, timeout, stream=False)</code>","text":"<p>Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.</p> <p>Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.</p> <p>Choose between streaming the result back, or storing it as a file in GCS (by providing a <code>targetUri</code>).</p> <p>Notice that you can specify the <code>targetContentType</code> if you want to convert to either of the supported file formats. E.g. your source could be a CSV file and the result could be a JSON file.</p> <p>Reduce transmission times by applying compression both to the source and target files. Specify <code>compression</code> if you want the result to be a zipped (and optionally) encrypted archive.</p> <p>Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being processed. Each rule defines a <code>pattern</code> (as a glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields. Rules are processed in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets of rules are linked to separate keysets.</p> <p>Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the <code>keysets</code> param) or use one of the predefined keys: <code>ssb-common-key-1</code> or <code>ssb-common-key-2</code>.</p> <p>See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile</p> <p>:param request_json: the request JSON to send to Dapla Pseudo Service :param file_path: path to a local file that should be depseudonymized :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files. :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/client.py</code> <pre><code>def repseudonymize_file(\n    self,\n    repseudonymize_request: RepseudonymizeFileRequest,\n    file_path: str,\n    timeout: int,\n    stream: bool = False,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize a file (JSON or CSV - or a zip with potentially multiple such files) by uploading the file.\n\n    Repseudonymization is done by first applying depseudonuymization and then pseudonymization to fields of the file.\n\n    Choose between streaming the result back, or storing it as a file in GCS (by providing a `targetUri`).\n\n    Notice that you can specify the `targetContentType` if you want to convert to either of the supported file\n    formats. E.g. your source could be a CSV file and the result could be a JSON file.\n\n    Reduce transmission times by applying compression both to the source and target files.\n    Specify `compression` if you want the result to be a zipped (and optionally) encrypted archive.\n\n    Repseudonymization will be applied according to a list of \"rules\" that target the fields of the file being\n    processed. Each rule defines a `pattern` (as a\n    glob (https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)) that identifies one or multiple\n    fields, and a `func` that will be applied to the matching fields. Rules are processed in the order they are\n    defined, and only the first matching rule will be applied (thus: rule ordering is important). Two sets of rules\n    are provided: one that defines how to depseudonymize and a second that defines how to pseudonymize. These sets\n    of rules are linked to separate keysets.\n\n    Pseudo rules will most times refer to crypto keys. You can provide your own keys to use (via the `keysets`\n    param) or use one of the predefined keys: `ssb-common-key-1` or `ssb-common-key-2`.\n\n    See https://dapla-pseudo-service.staging-bip-app.ssb.no/api-docs/redoc#tag/Pseudo-operations/operation/repseudonymizeFile\n\n    :param request_json: the request JSON to send to Dapla Pseudo Service\n    :param file_path: path to a local file that should be depseudonymized\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: set to true if the results should be chunked into pieces, e.g. if you operate on large files.\n    :return: repseudonymized data\n    \"\"\"\n    return self._process_file(\"repseudonymize\", repseudonymize_request, file_path, timeout, stream)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/","title":"models","text":"<p>This module defines helper classes and API models used to communicate with the Dapla Pseudo Service.</p>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.DaeadKeywordArgs","title":"<code>DaeadKeywordArgs</code>","text":"<p>             Bases: <code>PseudoFunctionArgs</code></p> <p>Representation of kwargs for the 'daead' function.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class DaeadKeywordArgs(PseudoFunctionArgs):\n    \"\"\"Representation of kwargs for the 'daead' function.\"\"\"\n\n    key_id: PredefinedKeys | str = PredefinedKeys.SSB_COMMON_KEY_1\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.DepseudonymizeFileRequest","title":"<code>DepseudonymizeFileRequest</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>DepseudonymizeFileRequest represents a request towards depseudonymize file API endpoints.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class DepseudonymizeFileRequest(APIModel):\n    \"\"\"DepseudonymizeFileRequest represents a request towards depseudonymize file API endpoints.\"\"\"\n\n    pseudo_config: PseudoConfig\n    target_uri: t.Optional[str] = None\n    target_content_type: t.Optional[str] = None\n    compression: t.Optional[TargetCompression] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.FF31KeywordArgs","title":"<code>FF31KeywordArgs</code>","text":"<p>             Bases: <code>PseudoFunctionArgs</code></p> <p>Representation of kwargs for the 'FF31' function.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class FF31KeywordArgs(PseudoFunctionArgs):\n    \"\"\"Representation of kwargs for the 'FF31' function.\"\"\"\n\n    key_id: PredefinedKeys | str = PredefinedKeys.PAPIS_COMMON_KEY_1\n    strategy: t.Optional[UnknownCharacterStrategy] = UnknownCharacterStrategy.SKIP\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.Field","title":"<code>Field</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>Field represents a targeted piece of data within a dataset or record.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>str</code> <p>field name or expression (e.g. a glob)</p> <code>mapping</code> <code>Optional[str]</code> <p>If defined, denotes a mapping transformation that should be applied before the operation in question, e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class Field(APIModel):\n    \"\"\"Field represents a targeted piece of data within a dataset or record.\n\n    Attributes:\n        pattern: field name or expression (e.g. a glob)\n        mapping: If defined, denotes a mapping transformation that should be applied before the operation in question,\n            e.g. \"sid\", meaning the field should be transformed to Stabil ID before being pseudonymized.\n    \"\"\"\n\n    pattern: str\n    mapping: t.Optional[str] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.KeyWrapper","title":"<code>KeyWrapper</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Hold information about a key, such as ID and keyset information.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class KeyWrapper(BaseModel):\n    \"\"\"Hold information about a key, such as ID and keyset information.\"\"\"\n\n    key_id: str = \"\"\n    keyset: t.Optional[PseudoKeyset] = None\n\n    def __init__(self, key: t.Optional[str | PseudoKeyset], **kwargs: t.Any):\n        \"\"\"Determine if a key is either a key reference (aka \"common key\") or a keyset.\n\n        If it is a key reference, treat this as the key's ID, else retrieve the key's ID from the keyset data structure.\n\n        :param key: either a key reference (as string) or a PseudoKeyset\n        \"\"\"\n        super().__init__(**kwargs)\n        if isinstance(key, str):\n            # Either we have a keyset json\n            if key.startswith(\"{\"):\n                pseudo_keyset = PseudoKeyset.parse_obj(json.loads(key))\n                self.key_id = pseudo_keyset.get_key_id()\n                self.keyset = pseudo_keyset\n            # Either or a \"key reference\" (i.e. an id of a \"common\" key)\n            else:\n                self.key_id = key\n                self.keyset = None\n        # Or we have an already parsed PseudoKeyset\n        elif isinstance(key, PseudoKeyset):\n            self.key_id = key.get_key_id()\n            self.keyset = key\n\n    def keyset_list(self) -&gt; t.Optional[t.List[PseudoKeyset]]:\n        \"\"\"Wrap the keyset in a list if it is defined - or return None if it is not.\"\"\"\n        return None if self.keyset is None else [self.keyset]\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.KeyWrapper.__init__","title":"<code>__init__(key, **kwargs)</code>","text":"<p>Determine if a key is either a key reference (aka \"common key\") or a keyset.</p> <p>If it is a key reference, treat this as the key's ID, else retrieve the key's ID from the keyset data structure.</p> <p>:param key: either a key reference (as string) or a PseudoKeyset</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def __init__(self, key: t.Optional[str | PseudoKeyset], **kwargs: t.Any):\n    \"\"\"Determine if a key is either a key reference (aka \"common key\") or a keyset.\n\n    If it is a key reference, treat this as the key's ID, else retrieve the key's ID from the keyset data structure.\n\n    :param key: either a key reference (as string) or a PseudoKeyset\n    \"\"\"\n    super().__init__(**kwargs)\n    if isinstance(key, str):\n        # Either we have a keyset json\n        if key.startswith(\"{\"):\n            pseudo_keyset = PseudoKeyset.parse_obj(json.loads(key))\n            self.key_id = pseudo_keyset.get_key_id()\n            self.keyset = pseudo_keyset\n        # Either or a \"key reference\" (i.e. an id of a \"common\" key)\n        else:\n            self.key_id = key\n            self.keyset = None\n    # Or we have an already parsed PseudoKeyset\n    elif isinstance(key, PseudoKeyset):\n        self.key_id = key.get_key_id()\n        self.keyset = key\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.KeyWrapper.keyset_list","title":"<code>keyset_list()</code>","text":"<p>Wrap the keyset in a list if it is defined - or return None if it is not.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def keyset_list(self) -&gt; t.Optional[t.List[PseudoKeyset]]:\n    \"\"\"Wrap the keyset in a list if it is defined - or return None if it is not.\"\"\"\n    return None if self.keyset is None else [self.keyset]\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.MapSidKeywordArgs","title":"<code>MapSidKeywordArgs</code>","text":"<p>             Bases: <code>PseudoFunctionArgs</code></p> <p>Representation of kwargs for the 'map-sid' function.</p> <p>Attributes:</p> Name Type Description <code>key_id</code> <code>PredefinedKeys | str</code> <p>The key to be used for pseudonomization</p> <code>snapshot_date</code> <code>date</code> <p>The timestamp for the version of the SID catalogue. If not specified, will choose the latest version.</p> <p>The format is: gmd where the bracketed parts represent year, month and day respectively Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class MapSidKeywordArgs(PseudoFunctionArgs):\n    \"\"\"Representation of kwargs for the 'map-sid' function.\n\n    Attributes:\n        key_id (PredefinedKeys | str): The key to be used for pseudonomization\n        snapshot_date (date): The timestamp for the version of the SID catalogue.\n            If not specified, will choose the latest version.\n\n            The format is:\n            g&lt;YYYY&gt;m&lt;MM&gt;d&lt;DD&gt;\n            where the bracketed parts represent year, month and day respectively\n    \"\"\"\n\n    key_id: PredefinedKeys | str = PredefinedKeys.PAPIS_COMMON_KEY_1\n    snapshot_date: t.Optional[date] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.Mimetypes","title":"<code>Mimetypes</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Mimetypes is an enum of supported mimetypes, for use in HTTP requests.</p> <p>As a proxy, this also defines the supported input file formats when reading from a file.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class Mimetypes(str, Enum):\n    \"\"\"Mimetypes is an enum of supported mimetypes, for use in HTTP requests.\n\n    As a proxy, this also defines the supported input file formats when reading from a file.\n    \"\"\"\n\n    JSON = \"application/json\"\n    CSV = \"text/csv\"\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoConfig","title":"<code>PseudoConfig</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>PseudoConfig is a container for rules and keysets.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudoConfig(APIModel):\n    \"\"\"PseudoConfig is a container for rules and keysets.\"\"\"\n\n    rules: t.List[PseudoRule]\n    keysets: t.Optional[t.List[PseudoKeyset]] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoFunction","title":"<code>PseudoFunction</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Formal representation of a pseudo function.</p> <p>Use to build up the string representation expected by pseudo service.</p> <p>Syntax: \"(=x, =y)\" <p>where , , etc. represents the keywords defined in PseudoFunctionArgs Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudoFunction(BaseModel):\n    \"\"\"Formal representation of a pseudo function.\n\n    Use to build up the string representation expected by pseudo service.\n\n    Syntax: \"&lt;function_type&gt;(&lt;kwarg_1&gt;=x, &lt;kwarg_2&gt;=y)\"\n\n    where &lt;kwarg_1&gt;, &lt;kwarg_2&gt;, etc. represents the keywords defined in PseudoFunctionArgs\n    \"\"\"\n\n    function_type: PseudoFunctionTypes\n    kwargs: DaeadKeywordArgs | FF31KeywordArgs | MapSidKeywordArgs | RedactArgs\n\n    def __str__(self) -&gt; str:\n        \"\"\"Create the function representation as expected by pseudo service.\"\"\"\n        return f\"{self.function_type}({self.kwargs})\"\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoFunction.__str__","title":"<code>__str__()</code>","text":"<p>Create the function representation as expected by pseudo service.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Create the function representation as expected by pseudo service.\"\"\"\n    return f\"{self.function_type}({self.kwargs})\"\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoFunctionArgs","title":"<code>PseudoFunctionArgs</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Representation of the possible keyword arguments</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudoFunctionArgs(BaseModel):\n    \"\"\"Representation of the possible keyword arguments\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"As a default, represent the fields of the subclasses as kwargs on the format 'k=v'.\"\"\"\n        return \",\".join(f\"{k}={v}\" for k, v in self.model_dump(by_alias=True).items() if v is not None)\n\n    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoFunctionArgs.__str__","title":"<code>__str__()</code>","text":"<p>As a default, represent the fields of the subclasses as kwargs on the format 'k=v'.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"As a default, represent the fields of the subclasses as kwargs on the format 'k=v'.\"\"\"\n    return \",\".join(f\"{k}={v}\" for k, v in self.model_dump(by_alias=True).items() if v is not None)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoKeyset","title":"<code>PseudoKeyset</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>PseudoKeyset represents a wrapped data encryption key (WDEK).</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudoKeyset(APIModel):\n    \"\"\"PseudoKeyset represents a wrapped data encryption key (WDEK).\"\"\"\n\n    encrypted_keyset: str\n    keyset_info: t.Dict[str, t.Any]\n    kek_uri: str\n\n    def get_key_id(self) -&gt; str:\n        \"\"\"ID of the keyset.\"\"\"\n        return str(self.keyset_info[\"primaryKeyId\"])\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoKeyset.get_key_id","title":"<code>get_key_id()</code>","text":"<p>ID of the keyset.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def get_key_id(self) -&gt; str:\n    \"\"\"ID of the keyset.\"\"\"\n    return str(self.keyset_info[\"primaryKeyId\"])\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoRule","title":"<code>PseudoRule</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>A <code>PseudoRule</code> defines a pattern, a transformation function, and optionally a friendly name of the rule.</p> <p>Each rule defines a glob pattern (see https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob) that identifies one or multiple fields, and a <code>func</code> that will be applied to the matching fields.</p> <p>Lists of PseudoRules are processed by the dapla-pseudo-service in the order they are defined, and only the first matching rule will be applied (thus: rule ordering is important).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <p>A friendly name of the rule. This is optional, but can be handy for debugging</p> <code>pattern</code> <code>str</code> <p>Glob expression, such as: <code>/**/{field1, field2, *navn}</code></p> <code>func</code> <code>PseudoFunction</code> <p>A transformation function, such as <code>tink-daead(&lt;keyname&gt;), redact(&lt;replacementstring&gt;) or fpe-anychar(&lt;keyname&gt;)</code></p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudoRule(APIModel):\n    \"\"\"A ``PseudoRule`` defines a pattern, a transformation function, and optionally a friendly name of the rule.\n\n    Each rule defines a glob pattern (see https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob)\n    that identifies one or multiple fields, and a `func` that will be applied to the matching fields.\n\n    Lists of PseudoRules are processed by the dapla-pseudo-service in the order they are defined, and only the first\n    matching rule will be applied (thus: rule ordering is important).\n\n    Attributes:\n        name: A friendly name of the rule. This is optional, but can be handy for debugging\n        pattern: Glob expression, such as: ``/**/{field1, field2, *navn}``\n        func: A transformation function, such as ``tink-daead(&lt;keyname&gt;), redact(&lt;replacementstring&gt;) or fpe-anychar(&lt;keyname&gt;)``\n    \"\"\"\n\n    name: t.Optional[str] = None\n    pattern: str\n    func: PseudoFunction\n\n    @field_serializer(\"func\")\n    def serialize_func(self, func: PseudoFunction, _info: FieldSerializationInfo) -&gt; str:\n        \"\"\"Explicit serialization of the 'func' field to coerce to string before serializing PseudoRule.\"\"\"\n        return str(func)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudoRule.serialize_func","title":"<code>serialize_func(func, _info)</code>","text":"<p>Explicit serialization of the 'func' field to coerce to string before serializing PseudoRule.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>@field_serializer(\"func\")\ndef serialize_func(self, func: PseudoFunction, _info: FieldSerializationInfo) -&gt; str:\n    \"\"\"Explicit serialization of the 'func' field to coerce to string before serializing PseudoRule.\"\"\"\n    return str(func)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.PseudonymizeFileRequest","title":"<code>PseudonymizeFileRequest</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>PseudonymizeFileRequest represents a request towards pseudonymize file API endpoints.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class PseudonymizeFileRequest(APIModel):\n    \"\"\"PseudonymizeFileRequest represents a request towards pseudonymize file API endpoints.\"\"\"\n\n    pseudo_config: PseudoConfig\n    target_uri: t.Optional[str] = None\n    target_content_type: Mimetypes\n    compression: t.Optional[TargetCompression] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.RedactArgs","title":"<code>RedactArgs</code>","text":"<p>             Bases: <code>PseudoFunctionArgs</code></p> <p>Representation of kwargs for the 'redact' function.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class RedactArgs(PseudoFunctionArgs):\n    \"\"\"Representation of kwargs for the 'redact' function.\"\"\"\n\n    replacement_string: str\n\n    def __str__(self) -&gt; str:\n        \"\"\"Overload the parent class. The redact function is expected as an arg, not kwarg.\n\n        I.e. 'redact(&lt;replacement_string&gt;)\n        \"\"\"\n        return self.replacement_string\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.RedactArgs.__str__","title":"<code>__str__()</code>","text":"<p>Overload the parent class. The redact function is expected as an arg, not kwarg.</p> <p>I.e. 'redact() Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Overload the parent class. The redact function is expected as an arg, not kwarg.\n\n    I.e. 'redact(&lt;replacement_string&gt;)\n    \"\"\"\n    return self.replacement_string\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.RepseudonymizeFileRequest","title":"<code>RepseudonymizeFileRequest</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>RepseudonymizeFileRequest represents a request towards repseudonymize file API endpoints.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class RepseudonymizeFileRequest(APIModel):\n    \"\"\"RepseudonymizeFileRequest represents a request towards repseudonymize file API endpoints.\"\"\"\n\n    source_pseudo_config: PseudoConfig\n    target_pseudo_config: PseudoConfig\n    target_uri: t.Optional[str] = None\n    target_content_type: str\n    compression: t.Optional[TargetCompression] = None\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/models/#dapla_pseudo.v1.models.TargetCompression","title":"<code>TargetCompression</code>","text":"<p>             Bases: <code>APIModel</code></p> <p>TargetCompression denotes if and how results from the API should be compressed and password encrypted.</p> Source code in <code>dapla_pseudo/v1/models.py</code> <pre><code>class TargetCompression(APIModel):\n    \"\"\"TargetCompression denotes if and how results from the API should be compressed and password encrypted.\"\"\"\n\n    password: str\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/ops/","title":"ops","text":"<p>Pseudonymization operations provided to Dapla end users.</p> <p>These operations aim to simplify the way Dapla users interact with the Dapla pseudo service. While the Dapla pseudo service API offers many advanced options, like detailed configuration of pseudo rules on a field-basis targeting hierarchical data structures, many users will be just fine with using one key and just listing the fields (of their flat data).</p>"},{"location":"reference/dapla_pseudo/v1/ops/#dapla_pseudo.v1.ops.depseudonymize","title":"<code>depseudonymize(file_path, fields, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Depseudonymize specified fields of a local file.</p> <p>This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users) will be allowed to depseudonymize data.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer_deid.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json :param fields: list of fields that should be depseudonymized :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: depseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def depseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Depseudonymize specified fields of a local file.\n\n    This is the inverse operation of \"pseudonymize\". Special privileges will be required (e.g. only whitelisted users)\n    will be allowed to depseudonymize data.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to pseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with depseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer_deid.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data-deid.json. Supported file formats: csv, json\n    :param fields: list of fields that should be depseudonymized\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: depseudonymized data\n    \"\"\"\n    content_type = mimetypes.MimeTypes().guess_type(file_path)[0]\n    k = KeyWrapper(key)\n    rules = _rules_of(fields=fields, sid_fields=[], key=k.key_id)\n    req = DepseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().depseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/ops/#dapla_pseudo.v1.ops.pseudonymize","title":"<code>pseudonymize(dataset, fields=None, sid_fields=None, sid_snapshot_date=None, key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Pseudonymize specified fields of a dataset.</p> The dataset may be supplied as <ul> <li>A local file on disk (string or Path)</li> <li>A file handle (io.BufferedReader)</li> <li>A Pandas dataframe</li> </ul> <p>Supported file formats: json, csv</p> <p>The <code>fields</code> and <code>sid_fields</code> lists specify what to pseudonymize. At least one of these fields must be specified. The list contents can either be plain field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply more advanced techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>For <code>fields</code>, the <code>daead</code> pseudonymization function is used. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>For <code>sid_fields</code>, the <code>map-sid</code> pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be compatible with those produced by the PAPIS project.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param dataset: path to file, file handle or dataframe :param fields: list of fields that should be pseudonymized :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD :param key: either named reference to a \"global\" key or a keyset json :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: pseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def pseudonymize(\n    dataset: DatasetDecl,\n    fields: t.Optional[t.List[FieldDecl]] = None,\n    sid_fields: t.Optional[t.List[str]] = None,\n    sid_snapshot_date: t.Optional[str | date] = None,\n    key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    r\"\"\"Pseudonymize specified fields of a dataset.\n\n    The dataset may be supplied as:\n        - A local file on disk (string or Path)\n        - A file handle (io.BufferedReader)\n        - A Pandas dataframe\n\n    Supported file formats: json, csv\n\n    The ``fields`` and ``sid_fields`` lists specify what to pseudonymize. At least one of these fields must be specified.\n    The list contents can either be plain field names (e.g. ``[\"some_field1\", \"another_field2\"]``, or you can apply\n    more advanced techniques, such as using wildcard characters (e.g. *Name) or slashes to target hierarchical fields\n    (e.g. **/path/to/hierarchicalStuff).\n\n    For ``fields``, the ``daead`` pseudonymization function is used. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    For ``sid_fields``, the ``map-sid`` pseudonymization function is used. This maps a f\u00f8dselsnummer to a \"stabil ID\" and\n    subsequently pseudonymizes the stabil ID using an FPE algorithm. Pseudonyms produced by this function are guaranteed to be\n    compatible with those produced by the PAPIS project.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with pseudonymize(\"./data/personer.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param dataset: path to file, file handle or dataframe\n    :param fields: list of fields that should be pseudonymized\n    :param sid_fields: list of fields that should be mapped to stabil ID and pseudonymized\n    :param sid_snapshot_date: Date representing SID-catalogue version to use. Latest if unspecified. Format: YYYY-MM-DD\n    :param key: either named reference to a \"global\" key or a keyset json\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: pseudonymized data\n    \"\"\"\n    if not fields and not sid_fields:\n        raise ValueError(\"At least one of fields and sid_fields must be specified.\")\n\n    # Avoid later type errors by making sure we have lists\n    if fields is None:\n        fields = []\n    if sid_fields is None:\n        sid_fields = []\n\n    file_handle: t.Optional[BinaryFileDecl] = None\n    name: t.Optional[str] = None\n    match dataset:\n        case str() | Path():\n            # File path\n            content_type = Mimetypes(magic.from_file(dataset, mime=True))\n        case pd.DataFrame():\n            # Dataframe\n            content_type = Mimetypes.JSON\n            file_handle = _dataframe_to_json(dataset, fields, sid_fields)\n        case io.BufferedReader():\n            # File handle\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            dataset.seek(0)\n            file_handle = dataset\n        case fsspec.spec.AbstractBufferedFile():\n            # This is a file handle to a remote storage system such as GCS.\n            # It provides random access for the underlying file-like data (without downloading the whole thing).\n            content_type = Mimetypes(magic.from_buffer(dataset.read(2048), mime=True))\n            name = dataset.path.split(\"/\")[-1] if hasattr(dataset, \"path\") else None\n            dataset.seek(0)\n            file_handle = io.BufferedReader(dataset)\n        case _:\n            raise ValueError(f\"Unsupported data type: {type(dataset)}. Supported types are {DatasetDecl}\")\n    k = KeyWrapper(key)\n    sid_func_kwargs = MapSidKeywordArgs(snapshot_date=convert_to_date(sid_snapshot_date)) if sid_fields else None\n    rules = _rules_of(\n        fields=fields,\n        sid_fields=sid_fields or [],\n        key=k.key_id,\n        sid_func_kwargs=sid_func_kwargs,\n    )\n    pseudonymize_request = PseudonymizeFileRequest(\n        pseudo_config=PseudoConfig(rules=rules, keysets=k.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    if file_handle is not None:\n        return _client().pseudonymize_file(pseudonymize_request, file_handle, stream=stream, name=name, timeout=timeout)\n    else:\n        return _client()._process_file(\n            \"pseudonymize\",\n            pseudonymize_request,\n            str(dataset),\n            stream=stream,\n            timeout=timeout,\n        )\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/ops/#dapla_pseudo.v1.ops.repseudonymize","title":"<code>repseudonymize(file_path, fields, source_key=PredefinedKeys.SSB_COMMON_KEY_1, target_key=PredefinedKeys.SSB_COMMON_KEY_1, timeout=TIMEOUT_DEFAULT, stream=True)</code>","text":"<p>Repseudonymize specified fields of a local, previously pseudonymized file.</p> <p>You will need to provide a crypto key for both the source data and a key that should be used for re-pseudonymization.</p> <p>Supported file formats: csv and json (both standard and \"new line delimited\" json)</p> <p>You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.</p> <p>The <code>fields</code> list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g. <code>[\"some_field1\", \"another_field2\"]</code>, or you can apply ninja-style techniques, such as using wildcard characters (e.g. Name) or slashes to target hierarchical fields (e.g. */path/to/hierarchicalStuff).</p> <p>Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key. You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\" will be used as default.</p> <p>It is possible to operate on the file in a streaming manner, e.g. like so:</p> <p>.. code-block:: python</p> <pre><code>with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n    with open(\"./data/personer.json\", 'wb') as f:\n        shutil.copyfileobj(res.raw, f)\n</code></pre> <p>:param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json :param fields: list of fields that should be pseudonymized :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization :param timeout: connection and read timeout, see     https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts :param stream: true if the results should be chunked into pieces (use for large data) :return: repseudonymized data</p> Source code in <code>dapla_pseudo/v1/ops.py</code> <pre><code>def repseudonymize(\n    file_path: str,\n    fields: t.List[FieldDecl],\n    source_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    target_key: t.Union[str, PseudoKeyset] = PredefinedKeys.SSB_COMMON_KEY_1,\n    timeout: int = TIMEOUT_DEFAULT,\n    stream: bool = True,\n) -&gt; requests.Response:\n    \"\"\"Repseudonymize specified fields of a local, previously pseudonymized file.\n\n    You will need to provide a crypto key for both the source data and a key that should be used for\n    re-pseudonymization.\n\n    Supported file formats: csv and json (both standard and \"new line delimited\" json)\n\n    You can alternatively send a zip-file containing one or many files of the supported file formats. The pseudo service\n    will unzip and process them sequentially. This can be handy if your file is large and/or split into multiple files.\n\n    The ``fields`` list specifies what to repseudonymize. This can either be a plain vanilla list of field names (e.g.\n    ``[\"some_field1\", \"another_field2\"]``, or you can apply ninja-style techniques, such as using wildcard characters\n    (e.g. *Name) or slashes to target hierarchical fields (e.g. **/path/to/hierarchicalStuff).\n\n    Pseudonymize uses the tink-daead crypto function underneath the hood. It requires a key.\n    You can choose to specify one of the predefined (\"globally available\") keys (\"ssb-common-key-1\" or\n    \"ssb-common-key-2\") or provide your own custom keyset. If you don't specify a key, the predefined \"ssb-common-key-1\"\n    will be used as default.\n\n    It is possible to operate on the file in a streaming manner, e.g. like so:\n\n    .. code-block:: python\n\n        with repseudonymize(\"./data/personer-deid.json\", fields=[\"fnr\", \"fornavn\", \"etternavn\"], stream=True) as res:\n            with open(\"./data/personer.json\", 'wb') as f:\n                shutil.copyfileobj(res.raw, f)\n\n    :param file_path: path to a local file, e.g. ./path/to/data.json. Supported file formats: csv, json\n    :param fields: list of fields that should be pseudonymized\n    :param source_key: either named reference to a \"global\" key or a keyset json - used for depseudonymization\n    :param target_key: either named reference to a \"global\" key or a keyset json - used for pseudonymization\n    :param timeout: connection and read timeout, see\n        https://requests.readthedocs.io/en/latest/user/advanced/?highlight=timeout#timeouts\n    :param stream: true if the results should be chunked into pieces (use for large data)\n    :return: repseudonymized data\n    \"\"\"\n    content_type = _content_type_of(file_path)\n    source_key_wrapper = KeyWrapper(source_key)\n    target_key_wrapper = KeyWrapper(target_key)\n    source_rules = _rules_of(fields=fields, sid_fields=[], key=source_key_wrapper.key_id)\n    target_rules = _rules_of(fields=fields, sid_fields=[], key=target_key_wrapper.key_id)\n    req = RepseudonymizeFileRequest(\n        source_pseudo_config=PseudoConfig(rules=source_rules, keysets=source_key_wrapper.keyset_list()),\n        target_pseudo_config=PseudoConfig(rules=target_rules, keysets=target_key_wrapper.keyset_list()),\n        target_content_type=content_type,\n        target_uri=None,\n        compression=None,\n    )\n\n    return _client().repseudonymize_file(req, file_path, stream=stream, timeout=timeout)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/supported_file_format/","title":"supported_file_format","text":"<p>Classes used to support reading of dataframes from file.</p>"},{"location":"reference/dapla_pseudo/v1/supported_file_format/#dapla_pseudo.v1.supported_file_format.SupportedOutputFileFormat","title":"<code>SupportedOutputFileFormat</code>","text":"<p>             Bases: <code>Enum</code></p> <p>SupportedOutputFileFormat contains the supported file formats when outputting the result to a file.</p> <p>Note that this does NOT describe the valid file extensions of input data when reading from a file.</p> Source code in <code>dapla_pseudo/v1/supported_file_format.py</code> <pre><code>class SupportedOutputFileFormat(Enum):\n    \"\"\"SupportedOutputFileFormat contains the supported file formats when outputting the result to a file.\n\n    Note that this does NOT describe the valid file extensions of _input_ data when reading from a file.\n    \"\"\"\n\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n    PARQUET = \"parquet\"\n\n    @classmethod\n    def _missing_(cls, value: object) -&gt; None:\n        raise ExtensionNotValidError(\n            f\"{value} is not a valid file format. Valid formats: %s\" % (\", \".join([repr(m.value) for m in cls]))\n        )\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/supported_file_format/#dapla_pseudo.v1.supported_file_format.read_to_pandas_df","title":"<code>read_to_pandas_df(supported_format, df_dataset, **kwargs)</code>","text":"<p>Reads a file with a supported file format to a Pandas Dataframe.</p> Source code in <code>dapla_pseudo/v1/supported_file_format.py</code> <pre><code>def read_to_pandas_df(\n    supported_format: SupportedOutputFileFormat,\n    df_dataset: BytesIO | Path,\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Reads a file with a supported file format to a Pandas Dataframe.\"\"\"\n    match supported_format:\n        case SupportedOutputFileFormat.CSV:\n            return pd.read_csv(df_dataset, sep=\";\", **kwargs)  # Pseudo Service CSV-separator is ';'\n        case SupportedOutputFileFormat.JSON:\n            return pd.read_json(df_dataset, **kwargs)\n        case SupportedOutputFileFormat.XML:\n            return pd.read_xml(df_dataset, **kwargs)\n        case SupportedOutputFileFormat.PARQUET:\n            return pd.read_parquet(df_dataset, **kwargs)\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/supported_file_format/#dapla_pseudo.v1.supported_file_format.read_to_polars_df","title":"<code>read_to_polars_df(supported_format, df_dataset, **kwargs)</code>","text":"<p>Reads a file with a supported file format to a Polars Dataframe.</p> Source code in <code>dapla_pseudo/v1/supported_file_format.py</code> <pre><code>def read_to_polars_df(\n    supported_format: SupportedOutputFileFormat,\n    df_dataset: BytesIO | Path,\n    **kwargs: Any,\n) -&gt; pl.DataFrame:\n    \"\"\"Reads a file with a supported file format to a Polars Dataframe.\"\"\"\n    match supported_format:\n        case SupportedOutputFileFormat.CSV:\n            return pl.read_csv(df_dataset, separator=\";\", **kwargs)  # Pseudo Service CSV-separator is ';'\n        case SupportedOutputFileFormat.JSON:\n            return pl.read_json(df_dataset, **kwargs)\n        case SupportedOutputFileFormat.PARQUET:\n            return pl.read_parquet(df_dataset, **kwargs)\n        case SupportedOutputFileFormat.XML:\n            raise ValueError(\"Unsupported file format for Polars: 'XML'. Use Pandas instead.\")\n</code></pre>"},{"location":"reference/dapla_pseudo/v1/supported_file_format/#dapla_pseudo.v1.supported_file_format.write_from_df","title":"<code>write_from_df(df, supported_format, file_path, **kwargs)</code>","text":"<p>Writes to a file with a supported file format from a Dataframe.</p> Source code in <code>dapla_pseudo/v1/supported_file_format.py</code> <pre><code>def write_from_df(\n    df: pl.DataFrame,\n    supported_format: SupportedOutputFileFormat,\n    file_path: Path | str,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Writes to a file with a supported file format from a Dataframe.\"\"\"\n    match supported_format:\n        case SupportedOutputFileFormat.CSV:\n            df.write_csv(file=file_path, **kwargs)\n        case SupportedOutputFileFormat.JSON:\n            df.write_json(file=file_path, **kwargs)\n        case SupportedOutputFileFormat.XML:\n            df.to_pandas().to_xml(file_path, **kwargs)\n        case SupportedOutputFileFormat.PARQUET:\n            df.write_parquet(file_path, **kwargs)\n</code></pre>"}]}